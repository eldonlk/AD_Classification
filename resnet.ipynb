{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "K.set_session(session)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io as sio\n",
    "from scipy import io\n",
    "\n",
    "from keras import layers\n",
    "from keras import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.resnet import ResNet50\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI = io.loadmat(\"ADNI_new.mat\")\n",
    "X, y = ADNI['X'][:,:,:,13:16], ADNI['y']\n",
    "X = X / np.max(X)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "io.savemat(\"ADNI_test.mat\", {'y_train' : y_train, 'X_train' : X_train, \n",
    "                             'y_test' : y_test, 'X_test' : X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 20s 0us/step\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100, 100, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 106, 106, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 50, 50, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 50, 50, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 50, 50, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 52, 52, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 25, 25, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 25, 25, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 25, 25, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 25, 25, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 25, 25, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 25, 25, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 25, 25, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 25, 25, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 25, 25, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 25, 25, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 25, 25, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 25, 25, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 25, 25, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 25, 25, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 25, 25, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 25, 25, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 25, 25, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 25, 25, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 25, 25, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 25, 25, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 25, 25, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 13, 13, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 13, 13, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 13, 13, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 13, 13, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 13, 13, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 13, 13, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 13, 13, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 13, 13, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 13, 13, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 13, 13, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 13, 13, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 13, 13, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 13, 13, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 13, 13, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 13, 13, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 13, 13, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 13, 13, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 13, 13, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 13, 13, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 13, 13, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 13, 13, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 13, 13, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 13, 13, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 7, 7, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 7, 7, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 7, 7, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 7, 7, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 7, 7, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 7, 7, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 7, 7, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 7, 7, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 7, 7, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 7, 7, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 7, 7, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 7, 7, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 7, 7, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 7, 7, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 7, 7, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 7, 7, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 7, 7, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 7, 7, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 7, 7, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 7, 7, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 7, 7, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 7, 7, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 7, 7, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 7, 7, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 7, 7, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 7, 7, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 7, 7, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 7, 7, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 7, 7, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 4, 4, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 4, 4, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 4, 4, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 4, 4, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 4, 4, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 4, 4, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 4, 4, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 4, 4, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 4, 4, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 4, 4, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 4, 4, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 4, 4, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pool (GlobalMaxPooling2D)   (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 3)            6147        max_pool[0][0]                   \n",
      "==================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 23,593,859\n",
      "Trainable params: 23,540,739\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_input = layers.Input([100, 100, 3])\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, \n",
    "                      pooling='max', input_tensor=image_input)\n",
    "x = base_model.output\n",
    "out = layers.Dense(3, activation='softmax', name='output_layer')(x)\n",
    "model = Model(inputs=image_input, outputs=out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 896 samples, validate on 224 samples\n",
      "Epoch 1/10\n",
      "896/896 [==============================] - 80s 90ms/step - loss: 6.7794 - accuracy: 0.4196 - val_loss: 14.8305 - val_accuracy: 0.2812\n",
      "Epoch 2/10\n",
      "896/896 [==============================] - 72s 81ms/step - loss: 6.0415 - accuracy: 0.5290 - val_loss: 359.2986 - val_accuracy: 0.2812\n",
      "Epoch 3/10\n",
      "896/896 [==============================] - 69s 77ms/step - loss: 2.8838 - accuracy: 0.6585 - val_loss: 278.4541 - val_accuracy: 0.2812\n",
      "Epoch 4/10\n",
      "896/896 [==============================] - 71s 79ms/step - loss: 2.5990 - accuracy: 0.6696 - val_loss: 831.4937 - val_accuracy: 0.3884\n",
      "Epoch 5/10\n",
      "896/896 [==============================] - 69s 77ms/step - loss: 1.4948 - accuracy: 0.7143 - val_loss: 266962.4682 - val_accuracy: 0.3884\n",
      "Epoch 6/10\n",
      "896/896 [==============================] - 70s 78ms/step - loss: 1.4659 - accuracy: 0.7578 - val_loss: 1141748.2790 - val_accuracy: 0.3884\n",
      "Epoch 7/10\n",
      "896/896 [==============================] - 69s 77ms/step - loss: 1.4462 - accuracy: 0.8304 - val_loss: 41.3328 - val_accuracy: 0.3884\n",
      "Epoch 8/10\n",
      "896/896 [==============================] - 69s 77ms/step - loss: 1.2550 - accuracy: 0.8348 - val_loss: 1.4731 - val_accuracy: 0.2812\n",
      "Epoch 9/10\n",
      "896/896 [==============================] - 69s 77ms/step - loss: 1.1397 - accuracy: 0.7277 - val_loss: 678.9486 - val_accuracy: 0.2812\n",
      "Epoch 10/10\n",
      "896/896 [==============================] - 70s 78ms/step - loss: 1.2225 - accuracy: 0.7801 - val_loss: 239.1138 - val_accuracy: 0.2812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x21d5ad898c8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=50, \n",
    "          epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3607142857142857"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test)\n",
    "np.mean(np.argmax(y_test, axis=1) == np.argmax(y_test_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01271223, 0.00306264, 0.98422515],\n",
       "       [0.01249018, 0.00312204, 0.98438776],\n",
       "       [0.01301413, 0.00309779, 0.9838881 ],\n",
       "       [0.01206621, 0.00306557, 0.9848682 ],\n",
       "       [0.01394314, 0.00314327, 0.98291355],\n",
       "       [0.01226588, 0.00295231, 0.98478186],\n",
       "       [0.01366814, 0.00304374, 0.9832881 ],\n",
       "       [0.01262902, 0.00304947, 0.9843215 ],\n",
       "       [0.01246031, 0.00304587, 0.9844938 ],\n",
       "       [0.01284688, 0.00306842, 0.9840847 ],\n",
       "       [0.0128315 , 0.00312442, 0.98404413],\n",
       "       [0.01258329, 0.00305579, 0.984361  ],\n",
       "       [0.01292625, 0.0030639 , 0.9840098 ],\n",
       "       [0.01228492, 0.00303646, 0.9846786 ],\n",
       "       [0.01280362, 0.00309012, 0.98410624],\n",
       "       [0.01245246, 0.00297765, 0.98456985],\n",
       "       [0.01256304, 0.00306055, 0.98437643],\n",
       "       [0.01261937, 0.00301449, 0.9843662 ],\n",
       "       [0.01272148, 0.00305904, 0.9842195 ],\n",
       "       [0.01343279, 0.00316611, 0.9834011 ],\n",
       "       [0.01348935, 0.00313343, 0.9833772 ],\n",
       "       [0.01235128, 0.00300573, 0.984643  ],\n",
       "       [0.01327047, 0.00307363, 0.9836559 ],\n",
       "       [0.01238814, 0.00307043, 0.9845414 ],\n",
       "       [0.01225166, 0.00298478, 0.98476356],\n",
       "       [0.01210993, 0.00300209, 0.98488796],\n",
       "       [0.01250967, 0.00302816, 0.98446214],\n",
       "       [0.01262816, 0.00310666, 0.9842652 ],\n",
       "       [0.01346877, 0.00319734, 0.9833339 ],\n",
       "       [0.01200758, 0.00294325, 0.9850492 ],\n",
       "       [0.01245355, 0.00301689, 0.9845295 ],\n",
       "       [0.01281598, 0.00303139, 0.9841526 ],\n",
       "       [0.01202762, 0.00299956, 0.98497283],\n",
       "       [0.01211794, 0.00301624, 0.9848658 ],\n",
       "       [0.01271316, 0.00306036, 0.9842265 ],\n",
       "       [0.01240904, 0.00305152, 0.98453945],\n",
       "       [0.01206405, 0.00295666, 0.98497933],\n",
       "       [0.01240603, 0.00293935, 0.98465455],\n",
       "       [0.01348702, 0.00308204, 0.983431  ],\n",
       "       [0.01284475, 0.00308087, 0.98407435],\n",
       "       [0.01277585, 0.00303987, 0.98418427],\n",
       "       [0.01269726, 0.00307016, 0.98423254],\n",
       "       [0.01212824, 0.00301941, 0.9848524 ],\n",
       "       [0.01266921, 0.00302276, 0.98430806],\n",
       "       [0.01240745, 0.00294752, 0.98464507],\n",
       "       [0.01255258, 0.0030332 , 0.9844142 ],\n",
       "       [0.0123694 , 0.00298186, 0.98464876],\n",
       "       [0.01287661, 0.00307749, 0.98404586],\n",
       "       [0.01311034, 0.00306045, 0.9838292 ],\n",
       "       [0.01311013, 0.00309804, 0.9837918 ],\n",
       "       [0.01260249, 0.00313214, 0.9842653 ],\n",
       "       [0.01220875, 0.00300524, 0.984786  ],\n",
       "       [0.01199055, 0.00299072, 0.9850188 ],\n",
       "       [0.01269086, 0.00301057, 0.9842986 ],\n",
       "       [0.01243489, 0.00307791, 0.98448724],\n",
       "       [0.01212522, 0.00295203, 0.98492277],\n",
       "       [0.01219445, 0.00296204, 0.98484343],\n",
       "       [0.01291342, 0.003069  , 0.98401755],\n",
       "       [0.01229867, 0.00310702, 0.98459435],\n",
       "       [0.01323806, 0.00315565, 0.9836062 ],\n",
       "       [0.0134593 , 0.00323803, 0.98330265],\n",
       "       [0.01256824, 0.00303705, 0.98439467],\n",
       "       [0.01275148, 0.00296304, 0.9842854 ],\n",
       "       [0.01292437, 0.00304083, 0.9840348 ],\n",
       "       [0.01219478, 0.00307707, 0.9847282 ],\n",
       "       [0.01222107, 0.00294549, 0.9848334 ],\n",
       "       [0.01258171, 0.00308492, 0.98433334],\n",
       "       [0.01196814, 0.00295276, 0.98507917],\n",
       "       [0.01262586, 0.00300315, 0.984371  ],\n",
       "       [0.01284423, 0.00307172, 0.98408407],\n",
       "       [0.01286505, 0.0030402 , 0.9840948 ],\n",
       "       [0.01231158, 0.00298922, 0.9846992 ],\n",
       "       [0.01286694, 0.00299948, 0.9841336 ],\n",
       "       [0.01277058, 0.00305401, 0.9841754 ],\n",
       "       [0.01217631, 0.00295465, 0.984869  ],\n",
       "       [0.01265716, 0.003028  , 0.98431486],\n",
       "       [0.01262256, 0.00308585, 0.98429155],\n",
       "       [0.01246619, 0.00299846, 0.9845353 ],\n",
       "       [0.01260013, 0.00300812, 0.9843918 ],\n",
       "       [0.0124618 , 0.00304418, 0.98449403],\n",
       "       [0.01239655, 0.00296205, 0.9846414 ],\n",
       "       [0.01305292, 0.00305678, 0.98389024],\n",
       "       [0.01233744, 0.00305518, 0.9846074 ],\n",
       "       [0.012127  , 0.00294001, 0.98493296],\n",
       "       [0.01299307, 0.00310293, 0.983904  ],\n",
       "       [0.01188523, 0.00294506, 0.9851697 ],\n",
       "       [0.01384687, 0.00313237, 0.9830208 ],\n",
       "       [0.01255391, 0.00311846, 0.9843276 ],\n",
       "       [0.01265678, 0.00305111, 0.98429215],\n",
       "       [0.01221268, 0.00298397, 0.9848033 ],\n",
       "       [0.0134124 , 0.00311076, 0.9834768 ],\n",
       "       [0.01294295, 0.00301501, 0.98404205],\n",
       "       [0.0127111 , 0.00316854, 0.9841203 ],\n",
       "       [0.01207104, 0.0030276 , 0.98490137],\n",
       "       [0.01407863, 0.00332792, 0.9825935 ],\n",
       "       [0.01236813, 0.0029946 , 0.9846372 ],\n",
       "       [0.01274799, 0.00314269, 0.98410934],\n",
       "       [0.01294429, 0.00308337, 0.9839723 ],\n",
       "       [0.01341891, 0.00316875, 0.9834124 ],\n",
       "       [0.01234846, 0.00298444, 0.98466706],\n",
       "       [0.01200986, 0.00305018, 0.98494   ],\n",
       "       [0.01323578, 0.00303945, 0.98372483],\n",
       "       [0.01348558, 0.00318431, 0.9833301 ],\n",
       "       [0.01274818, 0.00312125, 0.98413056],\n",
       "       [0.01223837, 0.00304086, 0.98472077],\n",
       "       [0.01248656, 0.00307406, 0.98443943],\n",
       "       [0.01222821, 0.0029527 , 0.98481905],\n",
       "       [0.01212118, 0.00298153, 0.9848973 ],\n",
       "       [0.01283338, 0.00313539, 0.9840312 ],\n",
       "       [0.01298085, 0.00304023, 0.98397887],\n",
       "       [0.01288603, 0.00312937, 0.98398465],\n",
       "       [0.01228958, 0.00311617, 0.9845942 ],\n",
       "       [0.0128603 , 0.00304747, 0.98409224],\n",
       "       [0.01282458, 0.00311011, 0.98406535],\n",
       "       [0.01231521, 0.00306643, 0.98461837],\n",
       "       [0.01255414, 0.00303149, 0.98441434],\n",
       "       [0.0122379 , 0.00299163, 0.9847705 ],\n",
       "       [0.01207364, 0.00308126, 0.9848451 ],\n",
       "       [0.01269567, 0.00315069, 0.9841537 ],\n",
       "       [0.01247125, 0.00290525, 0.9846235 ],\n",
       "       [0.01318784, 0.0032065 , 0.9836056 ],\n",
       "       [0.01235392, 0.00303632, 0.9846097 ],\n",
       "       [0.01261575, 0.00309941, 0.9842849 ],\n",
       "       [0.01215921, 0.00298903, 0.9848518 ],\n",
       "       [0.01298279, 0.00316044, 0.9838568 ],\n",
       "       [0.0121434 , 0.00300697, 0.9848496 ],\n",
       "       [0.01300507, 0.00328436, 0.9837106 ],\n",
       "       [0.01288411, 0.00304836, 0.98406756],\n",
       "       [0.01233836, 0.00296288, 0.9846987 ],\n",
       "       [0.01270562, 0.00303794, 0.98425645],\n",
       "       [0.01270074, 0.00304288, 0.98425645],\n",
       "       [0.01277551, 0.00311786, 0.98410666],\n",
       "       [0.01361176, 0.00316407, 0.98322415],\n",
       "       [0.01256005, 0.00303491, 0.9844051 ],\n",
       "       [0.0132558 , 0.00311412, 0.9836301 ],\n",
       "       [0.01289858, 0.00314911, 0.98395234],\n",
       "       [0.01271501, 0.00302482, 0.98426014],\n",
       "       [0.01268852, 0.00307127, 0.9842402 ],\n",
       "       [0.01223833, 0.00301657, 0.9847451 ],\n",
       "       [0.01283038, 0.00302797, 0.98414165],\n",
       "       [0.01257578, 0.00302275, 0.9844015 ],\n",
       "       [0.01254792, 0.00307267, 0.9843795 ],\n",
       "       [0.01266743, 0.00306567, 0.98426694],\n",
       "       [0.01184215, 0.00295341, 0.98520446],\n",
       "       [0.01230728, 0.00307994, 0.9846128 ],\n",
       "       [0.01227565, 0.00302995, 0.9846944 ],\n",
       "       [0.01321702, 0.00310937, 0.9836736 ],\n",
       "       [0.01244417, 0.00305889, 0.98449695],\n",
       "       [0.01248845, 0.00304875, 0.98446286],\n",
       "       [0.01316015, 0.00308571, 0.9837541 ],\n",
       "       [0.01236375, 0.00300783, 0.98462844],\n",
       "       [0.01255368, 0.00306267, 0.9843836 ],\n",
       "       [0.01289991, 0.00315695, 0.9839431 ],\n",
       "       [0.0123837 , 0.00299112, 0.9846252 ],\n",
       "       [0.01218589, 0.00300674, 0.9848074 ],\n",
       "       [0.01221504, 0.00307079, 0.9847142 ],\n",
       "       [0.01299773, 0.00310618, 0.98389614],\n",
       "       [0.01217066, 0.00289278, 0.98493654],\n",
       "       [0.01254684, 0.00300692, 0.9844462 ],\n",
       "       [0.01237937, 0.00299844, 0.9846222 ],\n",
       "       [0.0127123 , 0.00303524, 0.9842525 ],\n",
       "       [0.01211819, 0.00297181, 0.98490995],\n",
       "       [0.01239271, 0.00293221, 0.98467505],\n",
       "       [0.01289881, 0.0031074 , 0.98399377],\n",
       "       [0.01235544, 0.00299175, 0.9846528 ],\n",
       "       [0.01256064, 0.00297869, 0.98446065],\n",
       "       [0.01261234, 0.00304949, 0.9843382 ],\n",
       "       [0.01206154, 0.00299821, 0.98494023],\n",
       "       [0.01213667, 0.00298979, 0.98487353],\n",
       "       [0.01252835, 0.00304035, 0.9844313 ],\n",
       "       [0.0127785 , 0.00302031, 0.98420125],\n",
       "       [0.01309029, 0.0030806 , 0.9838291 ],\n",
       "       [0.01235032, 0.0030216 , 0.9846281 ],\n",
       "       [0.01276611, 0.00306807, 0.9841658 ],\n",
       "       [0.01291839, 0.00305729, 0.98402435],\n",
       "       [0.01277276, 0.00308407, 0.98414314],\n",
       "       [0.01295836, 0.00308066, 0.983961  ],\n",
       "       [0.01267869, 0.00317184, 0.9841495 ],\n",
       "       [0.0132631 , 0.00310555, 0.9836314 ],\n",
       "       [0.01224432, 0.00304338, 0.98471236],\n",
       "       [0.0124937 , 0.00297838, 0.9845279 ],\n",
       "       [0.01250498, 0.00303107, 0.9844639 ],\n",
       "       [0.01214456, 0.00299301, 0.98486245],\n",
       "       [0.01278225, 0.00308861, 0.9841292 ],\n",
       "       [0.01243219, 0.00302143, 0.9845464 ],\n",
       "       [0.01286545, 0.00306969, 0.9840649 ],\n",
       "       [0.01275552, 0.00303056, 0.98421395],\n",
       "       [0.01234514, 0.00309246, 0.98456234],\n",
       "       [0.01250665, 0.00305938, 0.98443395],\n",
       "       [0.01357357, 0.00307545, 0.98335093],\n",
       "       [0.0124831 , 0.00306877, 0.9844482 ],\n",
       "       [0.01258732, 0.00316934, 0.9842434 ],\n",
       "       [0.01243679, 0.00302973, 0.9845334 ],\n",
       "       [0.01299055, 0.00308955, 0.9839199 ],\n",
       "       [0.01298975, 0.00306249, 0.98394775],\n",
       "       [0.01367671, 0.00311415, 0.9832092 ],\n",
       "       [0.01228265, 0.00302414, 0.98469317],\n",
       "       [0.01259584, 0.00304773, 0.98435646],\n",
       "       [0.01245712, 0.00301841, 0.9845244 ],\n",
       "       [0.01269904, 0.00318979, 0.9841112 ],\n",
       "       [0.01242885, 0.0030819 , 0.9844892 ],\n",
       "       [0.01242895, 0.00299338, 0.9845777 ],\n",
       "       [0.01215529, 0.00297326, 0.98487145],\n",
       "       [0.01306062, 0.00306682, 0.9838726 ],\n",
       "       [0.01302929, 0.00308745, 0.9838832 ],\n",
       "       [0.01313718, 0.00313104, 0.98373175],\n",
       "       [0.01262334, 0.00305402, 0.9843226 ],\n",
       "       [0.01259021, 0.00303459, 0.9843752 ],\n",
       "       [0.01247018, 0.00302844, 0.9845013 ],\n",
       "       [0.01270601, 0.00311451, 0.98417956],\n",
       "       [0.01284004, 0.00304165, 0.98411834],\n",
       "       [0.01264488, 0.0030723 , 0.9842828 ],\n",
       "       [0.01292143, 0.00307448, 0.9840041 ],\n",
       "       [0.01283132, 0.00307131, 0.9840973 ],\n",
       "       [0.0127247 , 0.00305071, 0.98422456],\n",
       "       [0.01285146, 0.00308153, 0.98406696],\n",
       "       [0.01226044, 0.00299819, 0.9847414 ],\n",
       "       [0.0127095 , 0.00307491, 0.98421556],\n",
       "       [0.01353698, 0.00304889, 0.9834141 ],\n",
       "       [0.01210061, 0.00295567, 0.9849437 ],\n",
       "       [0.01250071, 0.00308929, 0.9844099 ],\n",
       "       [0.01200315, 0.00294303, 0.98505384],\n",
       "       [0.01252297, 0.00303095, 0.9844461 ],\n",
       "       [0.01196936, 0.00300633, 0.98502433],\n",
       "       [0.01253264, 0.00306237, 0.984405  ],\n",
       "       [0.01243548, 0.0030149 , 0.98454964],\n",
       "       [0.01239266, 0.00297272, 0.9846346 ],\n",
       "       [0.01287205, 0.00305244, 0.9840755 ],\n",
       "       [0.01255285, 0.00300139, 0.98444575],\n",
       "       [0.01254319, 0.00297521, 0.9844816 ],\n",
       "       [0.01283673, 0.00315232, 0.984011  ],\n",
       "       [0.01265062, 0.00302879, 0.9843205 ],\n",
       "       [0.01270373, 0.00311244, 0.9841838 ],\n",
       "       [0.01265999, 0.00300275, 0.9843373 ],\n",
       "       [0.01311531, 0.00304535, 0.9838394 ],\n",
       "       [0.01250865, 0.00306511, 0.98442626],\n",
       "       [0.01236648, 0.00303599, 0.98459756],\n",
       "       [0.0129845 , 0.00308967, 0.9839258 ],\n",
       "       [0.0124852 , 0.00301469, 0.98450005],\n",
       "       [0.01257648, 0.0031224 , 0.98430115],\n",
       "       [0.0122563 , 0.0030241 , 0.98471963],\n",
       "       [0.01255774, 0.0030721 , 0.98437023],\n",
       "       [0.01322954, 0.00314867, 0.9836218 ],\n",
       "       [0.01249908, 0.00301802, 0.98448294],\n",
       "       [0.01301815, 0.00312246, 0.9838594 ],\n",
       "       [0.01273707, 0.0030731 , 0.9841898 ],\n",
       "       [0.01224872, 0.00300538, 0.98474586],\n",
       "       [0.01308537, 0.00312849, 0.98378617],\n",
       "       [0.01217562, 0.0030494 , 0.984775  ],\n",
       "       [0.01234938, 0.0029841 , 0.98466647],\n",
       "       [0.01261444, 0.00308258, 0.984303  ],\n",
       "       [0.01236944, 0.00298608, 0.98464453],\n",
       "       [0.01245797, 0.00295357, 0.98458844],\n",
       "       [0.01225164, 0.00298967, 0.98475873],\n",
       "       [0.01214613, 0.00301587, 0.984838  ],\n",
       "       [0.01272135, 0.00306399, 0.98421466],\n",
       "       [0.01268254, 0.00307889, 0.98423856],\n",
       "       [0.01293858, 0.00308278, 0.9839787 ],\n",
       "       [0.01292482, 0.00306493, 0.9840103 ],\n",
       "       [0.01232615, 0.00308827, 0.9845856 ],\n",
       "       [0.01211302, 0.00294523, 0.9849417 ],\n",
       "       [0.01316057, 0.00309817, 0.9837413 ],\n",
       "       [0.01236869, 0.00302442, 0.98460686],\n",
       "       [0.01247162, 0.00308313, 0.9844453 ],\n",
       "       [0.01305742, 0.00313728, 0.98380536],\n",
       "       [0.01333616, 0.00307746, 0.9835864 ],\n",
       "       [0.0131029 , 0.00308433, 0.9838127 ],\n",
       "       [0.01228335, 0.00300627, 0.9847104 ],\n",
       "       [0.01346943, 0.00309013, 0.9834404 ],\n",
       "       [0.01309303, 0.00300956, 0.9838974 ],\n",
       "       [0.01273723, 0.00301577, 0.98424697],\n",
       "       [0.01315576, 0.00308851, 0.9837557 ],\n",
       "       [0.01268049, 0.00299901, 0.9843205 ],\n",
       "       [0.01310456, 0.00307163, 0.9838238 ],\n",
       "       [0.01195478, 0.00295577, 0.9850894 ],\n",
       "       [0.01330717, 0.00315195, 0.98354083],\n",
       "       [0.01323884, 0.00305857, 0.98370254],\n",
       "       [0.01262429, 0.00309512, 0.9842806 ],\n",
       "       [0.01241038, 0.00295085, 0.98463875],\n",
       "       [0.01238647, 0.0030024 , 0.9846111 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_max = np.max(X)\n",
    "train_gen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rescale=1/p_max,\n",
    "                               brightness_range=[0.8, 1.0], zoom_range=[0.9, 1.0])\n",
    "train_gen.fit(X_train)\n",
    "val_gen = ImageDataGenerator(rescale=1/p_max)\n",
    "val_gen.fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 102s 4s/step - loss: 2.7572 - accuracy: 0.3875 - val_loss: 968623.2500 - val_accuracy: 0.3607\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 97s 4s/step - loss: 2.1460 - accuracy: 0.4161 - val_loss: 4751.5703 - val_accuracy: 0.3607\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 92s 4s/step - loss: 2.0889 - accuracy: 0.4518 - val_loss: 1750.6793 - val_accuracy: 0.2857\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 91s 4s/step - loss: 1.9117 - accuracy: 0.4688 - val_loss: 317125.4688 - val_accuracy: 0.2857\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 91s 4s/step - loss: 1.4659 - accuracy: 0.4500 - val_loss: 4.3472 - val_accuracy: 0.3607\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 89s 4s/step - loss: 1.6006 - accuracy: 0.4554 - val_loss: 1.0982 - val_accuracy: 0.3607\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 90s 4s/step - loss: 1.2980 - accuracy: 0.4964 - val_loss: 1.3412 - val_accuracy: 0.3607\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 93s 4s/step - loss: 1.4146 - accuracy: 0.4857 - val_loss: 1.0119 - val_accuracy: 0.3607\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 91s 4s/step - loss: 1.2770 - accuracy: 0.5098 - val_loss: 1.1391 - val_accuracy: 0.3607\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 93s 4s/step - loss: 1.3750 - accuracy: 0.5161 - val_loss: 1.1737 - val_accuracy: 0.3607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x21d6d5c26c8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "model.fit_generator(train_gen.flow(X_train, y_train, batch_size=50), \n",
    "          epochs=10, validation_data=val_gen.flow(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI = io.loadmat(\"ADNI_new.mat\")\n",
    "X, y = ADNI['X'], ADNI['y']\n",
    "X = X / np.max(X)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 100, 30)      0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 100, 100, 32)      8672      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 100, 100, 32)      9248      \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 33, 33, 32)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 33, 33, 64)        18496     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 33, 33, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 10, 10, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 10, 10, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 10, 10, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 3, 3, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 3)                 3459      \n",
      "=================================================================\n",
      "Total params: 446,595\n",
      "Trainable params: 446,211\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "IMSIZE = 100\n",
    "input_layer = layers.Input([IMSIZE, IMSIZE, 30])\n",
    "\n",
    "# Block 1\n",
    "x = layers.Conv2D(32, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block1_conv1')(input_layer)\n",
    "x = layers.Conv2D(32, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block1_conv2')(x)\n",
    "x = layers.MaxPooling2D((3, 3), strides=(3, 3), name='block1_pool')(x)\n",
    "\n",
    "# Block 2\n",
    "x = layers.Conv2D(64, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block2_conv1')(x)\n",
    "x = layers.Conv2D(64, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block2_conv2')(x)\n",
    "x = layers.MaxPooling2D((5, 5), strides=(3, 3), name='block2_pool')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "# Block 3\n",
    "x = layers.Conv2D(128, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block3_conv1')(x)\n",
    "x = layers.Conv2D(128, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block3_conv2')(x)\n",
    "x = layers.Conv2D(128, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block3_conv3')(x)\n",
    "x = layers.MaxPooling2D((3, 3), strides=(3, 3), name='block3_pool')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "x = layers.Flatten(name='flatten')(x)\n",
    "#x = layers.Dense(512, activation='relu', name='fc1')(x)\n",
    "x = layers.Dense(3, activation='softmax', name='predictions')(x)\n",
    "output_layer = x\n",
    "\n",
    "model = Model(input_layer,output_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 896 samples, validate on 224 samples\n",
      "Epoch 1/75\n",
      "896/896 [==============================] - 12s 14ms/step - loss: 1.3930 - accuracy: 0.3382 - val_loss: 1.0965 - val_accuracy: 0.3527\n",
      "Epoch 2/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 1.0645 - accuracy: 0.4420 - val_loss: 1.1098 - val_accuracy: 0.3348\n",
      "Epoch 3/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 1.0140 - accuracy: 0.4844 - val_loss: 1.0948 - val_accuracy: 0.3438\n",
      "Epoch 4/75\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 0.9251 - accuracy: 0.5424 - val_loss: 1.0792 - val_accuracy: 0.3884\n",
      "Epoch 5/75\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 0.8560 - accuracy: 0.5893 - val_loss: 1.0899 - val_accuracy: 0.3839\n",
      "Epoch 6/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 0.8091 - accuracy: 0.6384 - val_loss: 1.0711 - val_accuracy: 0.3884\n",
      "Epoch 7/75\n",
      "896/896 [==============================] - 12s 14ms/step - loss: 0.6404 - accuracy: 0.7455 - val_loss: 1.0785 - val_accuracy: 0.4554\n",
      "Epoch 8/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 0.6111 - accuracy: 0.7388 - val_loss: 1.0570 - val_accuracy: 0.4241\n",
      "Epoch 9/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 0.4610 - accuracy: 0.8237 - val_loss: 1.0904 - val_accuracy: 0.4107\n",
      "Epoch 10/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 0.3203 - accuracy: 0.8917 - val_loss: 1.0481 - val_accuracy: 0.4018\n",
      "Epoch 11/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 0.1780 - accuracy: 0.9498 - val_loss: 1.0942 - val_accuracy: 0.5000\n",
      "Epoch 12/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 0.1365 - accuracy: 0.9621 - val_loss: 1.1243 - val_accuracy: 0.4420\n",
      "Epoch 13/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 0.1168 - accuracy: 0.9721 - val_loss: 1.1244 - val_accuracy: 0.4152\n",
      "Epoch 14/75\n",
      "896/896 [==============================] - 14s 15ms/step - loss: 0.0634 - accuracy: 0.9821 - val_loss: 1.0920 - val_accuracy: 0.3929\n",
      "Epoch 15/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 0.0345 - accuracy: 0.9955 - val_loss: 1.0958 - val_accuracy: 0.4107\n",
      "Epoch 16/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 1.0851 - val_accuracy: 0.4152\n",
      "Epoch 17/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.0759 - val_accuracy: 0.4152\n",
      "Epoch 18/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.0779 - val_accuracy: 0.4509\n",
      "Epoch 19/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.0717 - val_accuracy: 0.4732\n",
      "Epoch 20/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.0450 - val_accuracy: 0.4955\n",
      "Epoch 21/75\n",
      "896/896 [==============================] - 12s 14ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.0168 - val_accuracy: 0.5089\n",
      "Epoch 22/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.0019 - val_accuracy: 0.5446\n",
      "Epoch 23/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 9.9040e-04 - accuracy: 1.0000 - val_loss: 0.9639 - val_accuracy: 0.5848\n",
      "Epoch 24/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 8.7677e-04 - accuracy: 1.0000 - val_loss: 0.9427 - val_accuracy: 0.5848\n",
      "Epoch 25/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 8.3895e-04 - accuracy: 1.0000 - val_loss: 0.9135 - val_accuracy: 0.6071\n",
      "Epoch 26/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 7.6062e-04 - accuracy: 1.0000 - val_loss: 0.8842 - val_accuracy: 0.6429\n",
      "Epoch 27/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 7.6070e-04 - accuracy: 1.0000 - val_loss: 0.8574 - val_accuracy: 0.6295\n",
      "Epoch 28/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 6.4100e-04 - accuracy: 1.0000 - val_loss: 0.8413 - val_accuracy: 0.6518\n",
      "Epoch 29/75\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 6.4347e-04 - accuracy: 1.0000 - val_loss: 0.8258 - val_accuracy: 0.6741\n",
      "Epoch 30/75\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 5.7651e-04 - accuracy: 1.0000 - val_loss: 0.8122 - val_accuracy: 0.6607\n",
      "Epoch 31/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 5.5390e-04 - accuracy: 1.0000 - val_loss: 0.8088 - val_accuracy: 0.6830\n",
      "Epoch 32/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 5.1908e-04 - accuracy: 1.0000 - val_loss: 0.8067 - val_accuracy: 0.7009\n",
      "Epoch 33/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 5.0662e-04 - accuracy: 1.0000 - val_loss: 0.8142 - val_accuracy: 0.7143\n",
      "Epoch 34/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 4.9628e-04 - accuracy: 1.0000 - val_loss: 0.8180 - val_accuracy: 0.7188\n",
      "Epoch 35/75\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 4.6094e-04 - accuracy: 1.0000 - val_loss: 0.8385 - val_accuracy: 0.7188\n",
      "Epoch 36/75\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 4.5106e-04 - accuracy: 1.0000 - val_loss: 0.8522 - val_accuracy: 0.7188\n",
      "Epoch 37/75\n",
      "896/896 [==============================] - 14s 15ms/step - loss: 3.8852e-04 - accuracy: 1.0000 - val_loss: 0.8725 - val_accuracy: 0.7188\n",
      "Epoch 38/75\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 3.9833e-04 - accuracy: 1.0000 - val_loss: 0.8897 - val_accuracy: 0.7098\n",
      "Epoch 39/75\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 3.6938e-04 - accuracy: 1.0000 - val_loss: 0.9064 - val_accuracy: 0.7143\n",
      "Epoch 40/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 3.4453e-04 - accuracy: 1.0000 - val_loss: 0.9208 - val_accuracy: 0.7143\n",
      "Epoch 41/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 3.4640e-04 - accuracy: 1.0000 - val_loss: 0.9388 - val_accuracy: 0.7143\n",
      "Epoch 42/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 3.3315e-04 - accuracy: 1.0000 - val_loss: 0.9530 - val_accuracy: 0.7143\n",
      "Epoch 43/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 3.1060e-04 - accuracy: 1.0000 - val_loss: 0.9722 - val_accuracy: 0.6964\n",
      "Epoch 44/75\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 2.9604e-04 - accuracy: 1.0000 - val_loss: 0.9832 - val_accuracy: 0.7054\n",
      "Epoch 45/75\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 2.9283e-04 - accuracy: 1.0000 - val_loss: 0.9903 - val_accuracy: 0.6964\n",
      "Epoch 46/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 2.9678e-04 - accuracy: 1.0000 - val_loss: 1.0041 - val_accuracy: 0.7009\n",
      "Epoch 47/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 2.8258e-04 - accuracy: 1.0000 - val_loss: 1.0129 - val_accuracy: 0.7098\n",
      "Epoch 48/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 2.5318e-04 - accuracy: 1.0000 - val_loss: 1.0256 - val_accuracy: 0.7098\n",
      "Epoch 49/75\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 2.5784e-04 - accuracy: 1.0000 - val_loss: 1.0335 - val_accuracy: 0.7054\n",
      "Epoch 50/75\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 2.4830e-04 - accuracy: 1.0000 - val_loss: 1.0352 - val_accuracy: 0.7143\n",
      "Epoch 51/75\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 2.3659e-04 - accuracy: 1.0000 - val_loss: 1.0405 - val_accuracy: 0.7143\n",
      "Epoch 52/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 2.2467e-04 - accuracy: 1.0000 - val_loss: 1.0462 - val_accuracy: 0.7098\n",
      "Epoch 53/75\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 2.2313e-04 - accuracy: 1.0000 - val_loss: 1.0556 - val_accuracy: 0.7098\n",
      "Epoch 54/75\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 2.0996e-04 - accuracy: 1.0000 - val_loss: 1.0568 - val_accuracy: 0.7054\n",
      "Epoch 55/75\n",
      "896/896 [==============================] - 14s 15ms/step - loss: 2.0868e-04 - accuracy: 1.0000 - val_loss: 1.0599 - val_accuracy: 0.7054\n",
      "Epoch 56/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896/896 [==============================] - 14s 15ms/step - loss: 2.1547e-04 - accuracy: 1.0000 - val_loss: 1.0677 - val_accuracy: 0.7009\n",
      "Epoch 57/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 2.0308e-04 - accuracy: 1.0000 - val_loss: 1.0707 - val_accuracy: 0.7009\n",
      "Epoch 58/75\n",
      "896/896 [==============================] - 12s 14ms/step - loss: 1.9366e-04 - accuracy: 1.0000 - val_loss: 1.0740 - val_accuracy: 0.6964\n",
      "Epoch 59/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 1.8680e-04 - accuracy: 1.0000 - val_loss: 1.0771 - val_accuracy: 0.7009\n",
      "Epoch 60/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 1.9649e-04 - accuracy: 1.0000 - val_loss: 1.0794 - val_accuracy: 0.6920\n",
      "Epoch 61/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 1.8021e-04 - accuracy: 1.0000 - val_loss: 1.0831 - val_accuracy: 0.7009\n",
      "Epoch 62/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 1.8923e-04 - accuracy: 1.0000 - val_loss: 1.0760 - val_accuracy: 0.7054\n",
      "Epoch 63/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 1.6464e-04 - accuracy: 1.0000 - val_loss: 1.0809 - val_accuracy: 0.7009\n",
      "Epoch 64/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 1.7260e-04 - accuracy: 1.0000 - val_loss: 1.0832 - val_accuracy: 0.7054\n",
      "Epoch 65/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 1.6180e-04 - accuracy: 1.0000 - val_loss: 1.0901 - val_accuracy: 0.6920\n",
      "Epoch 66/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 1.7119e-04 - accuracy: 1.0000 - val_loss: 1.0925 - val_accuracy: 0.7009\n",
      "Epoch 67/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 1.6481e-04 - accuracy: 1.0000 - val_loss: 1.0918 - val_accuracy: 0.7009\n",
      "Epoch 68/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 1.5466e-04 - accuracy: 1.0000 - val_loss: 1.0939 - val_accuracy: 0.7009\n",
      "Epoch 69/75\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 1.4382e-04 - accuracy: 1.0000 - val_loss: 1.0997 - val_accuracy: 0.7009\n",
      "Epoch 70/75\n",
      "896/896 [==============================] - 12s 14ms/step - loss: 1.4682e-04 - accuracy: 1.0000 - val_loss: 1.1014 - val_accuracy: 0.7054\n",
      "Epoch 71/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 1.6256e-04 - accuracy: 1.0000 - val_loss: 1.0947 - val_accuracy: 0.7009\n",
      "Epoch 72/75\n",
      "896/896 [==============================] - 12s 14ms/step - loss: 1.4321e-04 - accuracy: 1.0000 - val_loss: 1.1049 - val_accuracy: 0.7054\n",
      "Epoch 73/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 1.3185e-04 - accuracy: 1.0000 - val_loss: 1.1058 - val_accuracy: 0.7009\n",
      "Epoch 74/75\n",
      "896/896 [==============================] - 12s 14ms/step - loss: 1.3768e-04 - accuracy: 1.0000 - val_loss: 1.1018 - val_accuracy: 0.7009\n",
      "Epoch 75/75\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 1.2692e-04 - accuracy: 1.0000 - val_loss: 1.1058 - val_accuracy: 0.7009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1d216ca0388>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=50, \n",
    "          epochs = 75, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 896 samples, validate on 224 samples\n",
      "Epoch 1/30\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 0.0536 - accuracy: 0.9844 - val_loss: 4.9997 - val_accuracy: 0.4286\n",
      "Epoch 2/30\n",
      "896/896 [==============================] - 11s 13ms/step - loss: 0.0824 - accuracy: 0.9710 - val_loss: 1.8705 - val_accuracy: 0.5938\n",
      "Epoch 3/30\n",
      "896/896 [==============================] - 11s 13ms/step - loss: 0.0682 - accuracy: 0.9799 - val_loss: 2.1197 - val_accuracy: 0.6339\n",
      "Epoch 4/30\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 0.0608 - accuracy: 0.9777 - val_loss: 1.9094 - val_accuracy: 0.5714\n",
      "Epoch 5/30\n",
      "896/896 [==============================] - 11s 13ms/step - loss: 0.0250 - accuracy: 0.9944 - val_loss: 3.4339 - val_accuracy: 0.4598\n",
      "Epoch 6/30\n",
      "896/896 [==============================] - 11s 13ms/step - loss: 0.0213 - accuracy: 0.9955 - val_loss: 2.9574 - val_accuracy: 0.5089\n",
      "Epoch 7/30\n",
      "896/896 [==============================] - 11s 13ms/step - loss: 0.0066 - accuracy: 0.9989 - val_loss: 2.0424 - val_accuracy: 0.5804\n",
      "Epoch 8/30\n",
      "896/896 [==============================] - 11s 13ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.3271 - val_accuracy: 0.6250\n",
      "Epoch 9/30\n",
      "896/896 [==============================] - 11s 13ms/step - loss: 9.9593e-04 - accuracy: 1.0000 - val_loss: 1.5112 - val_accuracy: 0.6652\n",
      "Epoch 10/30\n",
      "896/896 [==============================] - 11s 13ms/step - loss: 3.0432e-04 - accuracy: 1.0000 - val_loss: 1.4930 - val_accuracy: 0.7009\n",
      "Epoch 11/30\n",
      "896/896 [==============================] - 11s 13ms/step - loss: 2.0847e-04 - accuracy: 1.0000 - val_loss: 1.5085 - val_accuracy: 0.6920\n",
      "Epoch 12/30\n",
      "896/896 [==============================] - 11s 13ms/step - loss: 1.4280e-04 - accuracy: 1.0000 - val_loss: 1.4827 - val_accuracy: 0.6920\n",
      "Epoch 13/30\n",
      "896/896 [==============================] - 11s 13ms/step - loss: 1.2361e-04 - accuracy: 1.0000 - val_loss: 1.4679 - val_accuracy: 0.6920\n",
      "Epoch 14/30\n",
      "896/896 [==============================] - 11s 13ms/step - loss: 1.1293e-04 - accuracy: 1.0000 - val_loss: 1.4556 - val_accuracy: 0.6920\n",
      "Epoch 15/30\n",
      "896/896 [==============================] - 12s 13ms/step - loss: 1.0325e-04 - accuracy: 1.0000 - val_loss: 1.4424 - val_accuracy: 0.6920\n",
      "Epoch 16/30\n",
      "896/896 [==============================] - 12s 14ms/step - loss: 9.0046e-05 - accuracy: 1.0000 - val_loss: 1.4353 - val_accuracy: 0.6920\n",
      "Epoch 17/30\n",
      "896/896 [==============================] - 12s 14ms/step - loss: 8.2672e-05 - accuracy: 1.0000 - val_loss: 1.4306 - val_accuracy: 0.6920\n",
      "Epoch 18/30\n",
      "896/896 [==============================] - 12s 14ms/step - loss: 7.5011e-05 - accuracy: 1.0000 - val_loss: 1.4223 - val_accuracy: 0.6964\n",
      "Epoch 19/30\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 6.8251e-05 - accuracy: 1.0000 - val_loss: 1.4159 - val_accuracy: 0.6964\n",
      "Epoch 20/30\n",
      "896/896 [==============================] - 13s 15ms/step - loss: 7.0720e-05 - accuracy: 1.0000 - val_loss: 1.4115 - val_accuracy: 0.6920\n",
      "Epoch 21/30\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 6.0825e-05 - accuracy: 1.0000 - val_loss: 1.4089 - val_accuracy: 0.6964\n",
      "Epoch 22/30\n",
      "896/896 [==============================] - 12s 14ms/step - loss: 6.1230e-05 - accuracy: 1.0000 - val_loss: 1.4106 - val_accuracy: 0.6964\n",
      "Epoch 23/30\n",
      "896/896 [==============================] - 12s 14ms/step - loss: 5.5697e-05 - accuracy: 1.0000 - val_loss: 1.4123 - val_accuracy: 0.6920\n",
      "Epoch 24/30\n",
      "896/896 [==============================] - 12s 14ms/step - loss: 5.2715e-05 - accuracy: 1.0000 - val_loss: 1.4104 - val_accuracy: 0.6920\n",
      "Epoch 25/30\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 5.7176e-05 - accuracy: 1.0000 - val_loss: 1.4074 - val_accuracy: 0.7009\n",
      "Epoch 26/30\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 5.0123e-05 - accuracy: 1.0000 - val_loss: 1.4126 - val_accuracy: 0.6964\n",
      "Epoch 27/30\n",
      "896/896 [==============================] - 12s 14ms/step - loss: 4.9458e-05 - accuracy: 1.0000 - val_loss: 1.4143 - val_accuracy: 0.6964\n",
      "Epoch 28/30\n",
      "896/896 [==============================] - 12s 14ms/step - loss: 4.3255e-05 - accuracy: 1.0000 - val_loss: 1.4113 - val_accuracy: 0.6964\n",
      "Epoch 29/30\n",
      "896/896 [==============================] - 12s 14ms/step - loss: 4.3342e-05 - accuracy: 1.0000 - val_loss: 1.4067 - val_accuracy: 0.6964\n",
      "Epoch 30/30\n",
      "896/896 [==============================] - 13s 14ms/step - loss: 4.1966e-05 - accuracy: 1.0000 - val_loss: 1.4060 - val_accuracy: 0.6964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x21d27272608>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=50, \n",
    "          epochs= 30, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6928571428571428"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test)\n",
    "np.mean(np.argmax(y_test, axis=1) == np.argmax(y_test_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.04988346e-01, 7.48854578e-01, 4.61571105e-02],\n",
       "       [7.49915659e-01, 5.92215769e-02, 1.90862760e-01],\n",
       "       [3.18640680e-03, 2.28562057e-02, 9.73957360e-01],\n",
       "       [7.39132404e-01, 5.91316856e-02, 2.01735899e-01],\n",
       "       [7.93632527e-04, 1.98452413e-01, 8.00753951e-01],\n",
       "       [1.55496849e-02, 9.32507098e-01, 5.19433059e-02],\n",
       "       [4.73990619e-01, 2.39930704e-01, 2.86078691e-01],\n",
       "       [7.10559934e-02, 8.91954184e-01, 3.69898155e-02],\n",
       "       [9.77606535e-01, 1.92658883e-02, 3.12758726e-03],\n",
       "       [5.54366596e-03, 3.73650789e-01, 6.20805562e-01],\n",
       "       [3.05737495e-01, 4.36177790e-01, 2.58084655e-01],\n",
       "       [2.77307212e-01, 4.24099773e-01, 2.98593044e-01],\n",
       "       [8.21426332e-01, 1.67468399e-01, 1.11053204e-02],\n",
       "       [8.76179636e-02, 8.14302802e-01, 9.80791822e-02],\n",
       "       [4.13177580e-01, 5.57622790e-01, 2.91996486e-02],\n",
       "       [1.50120342e-02, 1.54098915e-02, 9.69578028e-01],\n",
       "       [7.33003399e-05, 9.37161408e-03, 9.90555108e-01],\n",
       "       [4.93476428e-02, 9.38062191e-01, 1.25900703e-02],\n",
       "       [9.54591215e-01, 4.05560695e-02, 4.85269586e-03],\n",
       "       [9.48990043e-03, 9.63051498e-01, 2.74585877e-02],\n",
       "       [8.40529148e-03, 9.77556050e-01, 1.40385441e-02],\n",
       "       [2.12204233e-01, 7.42450774e-01, 4.53449786e-02],\n",
       "       [4.69823321e-03, 4.24807295e-02, 9.52821076e-01],\n",
       "       [7.42002308e-01, 2.54645735e-01, 3.35189817e-03],\n",
       "       [5.43555796e-01, 4.43023980e-01, 1.34202205e-02],\n",
       "       [2.22099602e-01, 6.16573036e-01, 1.61327451e-01],\n",
       "       [2.86520243e-01, 4.72574949e-01, 2.40904853e-01],\n",
       "       [8.80381227e-01, 5.52330948e-02, 6.43856823e-02],\n",
       "       [2.24323735e-01, 6.57761514e-01, 1.17914818e-01],\n",
       "       [2.97646015e-03, 9.68180835e-01, 2.88427752e-02],\n",
       "       [1.85476199e-01, 7.24271595e-01, 9.02521014e-02],\n",
       "       [4.63416874e-01, 9.75188985e-02, 4.39064264e-01],\n",
       "       [3.75736458e-03, 5.55832349e-02, 9.40659463e-01],\n",
       "       [8.67699742e-01, 1.31829649e-01, 4.70493309e-04],\n",
       "       [2.94137117e-03, 4.64108258e-01, 5.32950401e-01],\n",
       "       [1.28558342e-04, 3.64526473e-02, 9.63418782e-01],\n",
       "       [3.05979513e-02, 9.59525108e-01, 9.87698231e-03],\n",
       "       [5.61788853e-04, 6.49027005e-02, 9.34535563e-01],\n",
       "       [1.30430562e-03, 3.33584785e-01, 6.65110946e-01],\n",
       "       [7.63989925e-01, 1.99846029e-01, 3.61640528e-02],\n",
       "       [9.79666710e-01, 1.61991920e-02, 4.13408643e-03],\n",
       "       [3.16221900e-02, 8.77166569e-01, 9.12112147e-02],\n",
       "       [2.11835533e-04, 7.89639950e-01, 2.10148290e-01],\n",
       "       [1.63319409e-01, 8.19296420e-01, 1.73841733e-02],\n",
       "       [1.36975665e-03, 5.70798695e-01, 4.27831620e-01],\n",
       "       [5.48882902e-01, 4.08607066e-01, 4.25100997e-02],\n",
       "       [3.46332300e-03, 5.03504239e-02, 9.46186244e-01],\n",
       "       [1.81438681e-03, 9.95268762e-01, 2.91676214e-03],\n",
       "       [1.54751120e-02, 3.87842171e-02, 9.45740640e-01],\n",
       "       [3.19722248e-03, 2.37565134e-02, 9.73046303e-01],\n",
       "       [1.88972563e-01, 6.96934760e-01, 1.14092611e-01],\n",
       "       [6.63001776e-01, 3.27495456e-01, 9.50284209e-03],\n",
       "       [7.32518360e-03, 7.97027409e-01, 1.95647418e-01],\n",
       "       [2.09280327e-02, 9.76017058e-01, 3.05490592e-03],\n",
       "       [4.64903146e-01, 5.14385223e-01, 2.07115747e-02],\n",
       "       [3.40009965e-02, 8.67052734e-01, 9.89461914e-02],\n",
       "       [2.16363534e-01, 5.47676206e-01, 2.35960260e-01],\n",
       "       [4.92878519e-02, 9.43465233e-01, 7.24691059e-03],\n",
       "       [9.97045577e-01, 2.57842243e-03, 3.76042823e-04],\n",
       "       [1.39537910e-02, 5.53057678e-02, 9.30740416e-01],\n",
       "       [5.17124659e-04, 2.11031422e-01, 7.88451493e-01],\n",
       "       [9.13797989e-02, 3.73680502e-01, 5.34939706e-01],\n",
       "       [2.69871410e-02, 9.68695462e-01, 4.31745453e-03],\n",
       "       [4.75778664e-03, 5.41550100e-01, 4.53692138e-01],\n",
       "       [2.98466126e-04, 2.26666164e-02, 9.77034926e-01],\n",
       "       [6.67924166e-01, 2.38126516e-01, 9.39492509e-02],\n",
       "       [2.02797214e-03, 4.23701713e-03, 9.93735015e-01],\n",
       "       [2.60622948e-02, 7.66143724e-02, 8.97323310e-01],\n",
       "       [5.14833303e-03, 9.88535583e-01, 6.31617941e-03],\n",
       "       [2.98596313e-03, 8.74615312e-02, 9.09552515e-01],\n",
       "       [9.44400614e-04, 4.15015891e-02, 9.57554042e-01],\n",
       "       [7.13475107e-04, 1.07999602e-02, 9.88486528e-01],\n",
       "       [1.46286539e-03, 2.36314144e-02, 9.74905670e-01],\n",
       "       [5.67567423e-02, 6.82274163e-01, 2.60969102e-01],\n",
       "       [2.05861121e-01, 7.75931060e-01, 1.82077885e-02],\n",
       "       [6.91551686e-05, 3.81529927e-02, 9.61777806e-01],\n",
       "       [7.40433633e-01, 2.24670827e-01, 3.48956138e-02],\n",
       "       [8.78304958e-01, 7.13281929e-02, 5.03668487e-02],\n",
       "       [3.35698365e-04, 1.51650414e-01, 8.48013937e-01],\n",
       "       [5.12406707e-01, 4.85099882e-01, 2.49344506e-03],\n",
       "       [9.81895369e-04, 6.04117334e-01, 3.94900769e-01],\n",
       "       [5.33945300e-02, 5.54012775e-01, 3.92592639e-01],\n",
       "       [5.09071397e-03, 9.46961701e-01, 4.79476303e-02],\n",
       "       [2.85284012e-04, 9.92713988e-01, 7.00063631e-03],\n",
       "       [4.78030089e-03, 3.93341780e-02, 9.55885589e-01],\n",
       "       [7.27683008e-02, 9.05154765e-01, 2.20769141e-02],\n",
       "       [1.81210749e-02, 3.39148551e-01, 6.42730415e-01],\n",
       "       [6.50284410e-01, 1.26929786e-02, 3.37022603e-01],\n",
       "       [1.18130013e-01, 8.65132987e-01, 1.67370141e-02],\n",
       "       [7.20279757e-03, 1.04860766e-02, 9.82311130e-01],\n",
       "       [1.64665818e-01, 8.20714056e-01, 1.46201812e-02],\n",
       "       [1.28747541e-02, 9.47751820e-01, 3.93735282e-02],\n",
       "       [5.46131074e-01, 4.04518455e-01, 4.93504629e-02],\n",
       "       [6.34719478e-03, 9.52968180e-01, 4.06846814e-02],\n",
       "       [9.64060519e-03, 1.83522731e-01, 8.06836665e-01],\n",
       "       [4.08297449e-01, 5.48008442e-01, 4.36941050e-02],\n",
       "       [1.30404532e-01, 8.67956817e-01, 1.63870153e-03],\n",
       "       [9.17569041e-01, 1.96930650e-03, 8.04616213e-02],\n",
       "       [4.54644337e-02, 9.17081118e-01, 3.74544747e-02],\n",
       "       [5.77392578e-01, 4.21655595e-01, 9.51760041e-04],\n",
       "       [4.65830028e-01, 5.08674979e-01, 2.54949387e-02],\n",
       "       [1.32740410e-02, 4.20948863e-02, 9.44631040e-01],\n",
       "       [6.27162933e-01, 3.57171476e-01, 1.56655163e-02],\n",
       "       [3.47671926e-01, 6.51528776e-01, 7.99280184e-04],\n",
       "       [5.47778249e-01, 4.50354427e-01, 1.86729222e-03],\n",
       "       [8.33455265e-01, 1.53980598e-01, 1.25640640e-02],\n",
       "       [2.46714577e-02, 1.10208787e-01, 8.65119755e-01],\n",
       "       [2.75401250e-02, 2.93803453e-01, 6.78656340e-01],\n",
       "       [2.05115620e-02, 6.51375949e-01, 3.28112572e-01],\n",
       "       [1.64029077e-02, 9.68167722e-01, 1.54293561e-02],\n",
       "       [5.57746626e-02, 8.26805234e-01, 1.17420100e-01],\n",
       "       [5.49185872e-02, 8.98395479e-01, 4.66859043e-02],\n",
       "       [6.85295388e-02, 5.91828644e-01, 3.39641839e-01],\n",
       "       [8.80622685e-01, 7.91448727e-02, 4.02324684e-02],\n",
       "       [1.23014964e-01, 2.33946368e-02, 8.53590369e-01],\n",
       "       [5.09764373e-01, 2.40042821e-01, 2.50192702e-01],\n",
       "       [7.86119048e-03, 8.94022822e-01, 9.81160924e-02],\n",
       "       [1.06412126e-03, 3.15758348e-01, 6.83177531e-01],\n",
       "       [5.70897102e-01, 3.53413194e-01, 7.56897181e-02],\n",
       "       [6.57343399e-03, 3.67630720e-02, 9.56663489e-01],\n",
       "       [8.38464022e-01, 1.57889262e-01, 3.64664826e-03],\n",
       "       [1.78932533e-01, 4.75741655e-01, 3.45325768e-01],\n",
       "       [8.99751067e-01, 9.96594205e-02, 5.89510018e-04],\n",
       "       [9.44064081e-01, 4.91718315e-02, 6.76411018e-03],\n",
       "       [8.65829647e-01, 8.67883116e-03, 1.25491485e-01],\n",
       "       [8.72131646e-01, 1.19885989e-01, 7.98231270e-03],\n",
       "       [2.37627655e-01, 7.38249123e-01, 2.41231881e-02],\n",
       "       [6.32434070e-01, 9.62414443e-02, 2.71324515e-01],\n",
       "       [5.47999330e-02, 4.23770785e-01, 5.21429241e-01],\n",
       "       [8.47268164e-01, 1.02008708e-01, 5.07231206e-02],\n",
       "       [1.37861427e-02, 2.35502452e-01, 7.50711441e-01],\n",
       "       [8.04318581e-03, 9.46290195e-01, 4.56665792e-02],\n",
       "       [1.58459157e-01, 5.52196801e-01, 2.89344043e-01],\n",
       "       [3.00511494e-02, 3.69168252e-01, 6.00780606e-01],\n",
       "       [1.05089112e-03, 4.81643640e-02, 9.50784683e-01],\n",
       "       [1.59150347e-01, 3.83861035e-01, 4.56988543e-01],\n",
       "       [4.60651092e-04, 4.36253846e-03, 9.95176792e-01],\n",
       "       [2.00039465e-02, 9.75194216e-01, 4.80182003e-03],\n",
       "       [7.73135573e-03, 4.89372350e-02, 9.43331361e-01],\n",
       "       [7.59046059e-04, 2.15761006e-01, 7.83479989e-01],\n",
       "       [5.96904993e-01, 1.57564148e-01, 2.45530829e-01],\n",
       "       [7.17768669e-01, 1.58655912e-01, 1.23575352e-01],\n",
       "       [1.79607794e-02, 9.64827955e-01, 1.72112547e-02],\n",
       "       [9.97982100e-02, 8.98257375e-01, 1.94443041e-03],\n",
       "       [6.35109603e-01, 2.93203533e-01, 7.16868341e-02],\n",
       "       [1.72857894e-03, 9.51538980e-01, 4.67324071e-02],\n",
       "       [2.40835071e-01, 3.21186781e-01, 4.37978148e-01],\n",
       "       [7.86941685e-03, 2.25593671e-01, 7.66536891e-01],\n",
       "       [7.14677498e-02, 1.69171542e-01, 7.59360731e-01],\n",
       "       [1.32340258e-02, 9.35995638e-01, 5.07704206e-02],\n",
       "       [1.18329450e-02, 9.75520015e-01, 1.26470374e-02],\n",
       "       [1.00080803e-01, 7.27545619e-01, 1.72373563e-01],\n",
       "       [5.95209122e-01, 3.98964256e-01, 5.82659477e-03],\n",
       "       [1.55978184e-03, 2.55743461e-03, 9.95882750e-01],\n",
       "       [6.10117793e-01, 3.69042069e-01, 2.08401736e-02],\n",
       "       [1.42861845e-03, 8.71037075e-04, 9.97700393e-01],\n",
       "       [5.04091978e-02, 2.10170448e-01, 7.39420295e-01],\n",
       "       [9.23600912e-01, 4.82424535e-02, 2.81566996e-02],\n",
       "       [1.75891444e-01, 7.57296324e-01, 6.68121949e-02],\n",
       "       [9.78373110e-01, 1.08578708e-02, 1.07690394e-02],\n",
       "       [2.37791806e-01, 1.78968340e-01, 5.83239853e-01],\n",
       "       [6.12628274e-03, 1.31787837e-01, 8.62085938e-01],\n",
       "       [1.51254458e-03, 2.43667029e-02, 9.74120796e-01],\n",
       "       [8.86241049e-02, 1.07760951e-01, 8.03614974e-01],\n",
       "       [1.04735702e-01, 8.79388869e-01, 1.58754867e-02],\n",
       "       [3.36525030e-03, 9.81374443e-01, 1.52602522e-02],\n",
       "       [2.90765882e-01, 6.96758568e-01, 1.24755427e-02],\n",
       "       [9.93564546e-01, 2.84233619e-03, 3.59305623e-03],\n",
       "       [1.00929029e-02, 9.87275422e-01, 2.63164355e-03],\n",
       "       [8.50323558e-01, 3.45533453e-02, 1.15123034e-01],\n",
       "       [1.63672994e-05, 2.79646157e-03, 9.97187197e-01],\n",
       "       [4.64134216e-02, 2.70582028e-02, 9.26528335e-01],\n",
       "       [6.32584468e-02, 8.46878886e-01, 8.98626670e-02],\n",
       "       [2.33114526e-01, 6.34195209e-01, 1.32690236e-01],\n",
       "       [9.59189143e-03, 7.87697792e-01, 2.02710316e-01],\n",
       "       [4.01771720e-03, 9.67646837e-01, 2.83354279e-02],\n",
       "       [9.73181784e-01, 1.19745070e-02, 1.48437005e-02],\n",
       "       [6.60834968e-01, 3.22304159e-01, 1.68608986e-02],\n",
       "       [9.47611749e-01, 2.75674369e-02, 2.48208512e-02],\n",
       "       [6.28659487e-01, 1.36243314e-01, 2.35097304e-01],\n",
       "       [7.57852895e-03, 3.84025201e-02, 9.54018891e-01],\n",
       "       [6.58471942e-01, 3.14748287e-01, 2.67797988e-02],\n",
       "       [2.10846346e-02, 9.49062943e-01, 2.98523288e-02],\n",
       "       [8.14924669e-03, 2.66698778e-01, 7.25152016e-01],\n",
       "       [8.53435509e-03, 6.66915059e-01, 3.24550569e-01],\n",
       "       [2.24183071e-02, 1.26551032e-01, 8.51030648e-01],\n",
       "       [2.45855260e-03, 9.26246107e-01, 7.12953359e-02],\n",
       "       [2.71176808e-02, 7.44857132e-01, 2.28025213e-01],\n",
       "       [7.99179852e-01, 1.97601125e-01, 3.21902568e-03],\n",
       "       [1.16835292e-02, 1.13485366e-01, 8.74831140e-01],\n",
       "       [5.21575272e-01, 2.83966899e-01, 1.94457814e-01],\n",
       "       [1.23647964e-02, 9.80929136e-01, 6.70600682e-03],\n",
       "       [8.46322477e-01, 1.14231557e-01, 3.94460112e-02],\n",
       "       [7.95111477e-01, 3.96226207e-03, 2.00926259e-01],\n",
       "       [6.63771003e-04, 2.37378161e-02, 9.75598454e-01],\n",
       "       [2.62152730e-03, 4.76585448e-01, 5.20792961e-01],\n",
       "       [5.66166677e-02, 9.33590889e-01, 9.79241449e-03],\n",
       "       [7.94417977e-01, 1.85049981e-01, 2.05320399e-02],\n",
       "       [1.04224973e-03, 1.67955160e-02, 9.82162178e-01],\n",
       "       [5.88626042e-02, 6.65657148e-02, 8.74571681e-01],\n",
       "       [3.84972632e-01, 6.03531003e-01, 1.14963790e-02],\n",
       "       [3.05630397e-02, 8.92593682e-01, 7.68432021e-02],\n",
       "       [6.20788708e-03, 8.93986449e-02, 9.04393435e-01],\n",
       "       [9.77027416e-02, 8.18039238e-01, 8.42580274e-02],\n",
       "       [5.37915789e-02, 9.08229828e-01, 3.79785337e-02],\n",
       "       [2.59940736e-02, 6.91005647e-01, 2.83000290e-01],\n",
       "       [7.65956819e-01, 2.20862046e-01, 1.31811257e-02],\n",
       "       [8.80792677e-01, 1.01509169e-01, 1.76982041e-02],\n",
       "       [1.98964283e-01, 7.04263747e-01, 9.67719480e-02],\n",
       "       [3.36185023e-02, 9.62354481e-01, 4.02704580e-03],\n",
       "       [7.81134665e-02, 8.86856377e-01, 3.50300334e-02],\n",
       "       [3.06837242e-02, 7.47283340e-01, 2.22032979e-01],\n",
       "       [1.64089918e-01, 7.27369726e-01, 1.08540371e-01],\n",
       "       [1.35094131e-04, 8.36266950e-03, 9.91502285e-01],\n",
       "       [6.35825276e-01, 3.06728005e-01, 5.74467778e-02],\n",
       "       [8.50343823e-01, 1.38656184e-01, 1.09999068e-02],\n",
       "       [5.61501868e-02, 7.41923332e-01, 2.01926470e-01],\n",
       "       [2.58610882e-02, 9.20867741e-01, 5.32710627e-02],\n",
       "       [3.18030128e-03, 9.30677131e-02, 9.03752029e-01],\n",
       "       [2.06618816e-01, 6.88455641e-01, 1.04925573e-01],\n",
       "       [8.87654498e-02, 2.03006733e-02, 8.90933931e-01],\n",
       "       [7.20629841e-02, 9.26328421e-01, 1.60854030e-03],\n",
       "       [1.07532153e-02, 1.89650282e-01, 7.99596429e-01],\n",
       "       [3.91768143e-02, 9.58899260e-01, 1.92396354e-03],\n",
       "       [4.81177568e-01, 5.14424741e-01, 4.39771218e-03],\n",
       "       [2.20977236e-03, 9.29880515e-03, 9.88491416e-01],\n",
       "       [4.55346610e-03, 9.34326649e-01, 6.11198582e-02],\n",
       "       [3.54963876e-02, 6.91332594e-02, 8.95370424e-01],\n",
       "       [9.86309373e-04, 9.68675852e-01, 3.03378142e-02],\n",
       "       [9.71502542e-01, 2.32331902e-02, 5.26421564e-03],\n",
       "       [4.12811991e-03, 9.94169950e-01, 1.70192751e-03],\n",
       "       [1.84348866e-01, 4.25860733e-01, 3.89790386e-01],\n",
       "       [6.11322880e-01, 3.41179550e-01, 4.74975146e-02],\n",
       "       [3.45631689e-02, 3.49384218e-01, 6.16052568e-01],\n",
       "       [5.93761839e-02, 9.36288953e-01, 4.33483208e-03],\n",
       "       [1.10945322e-01, 1.53420344e-01, 7.35634387e-01],\n",
       "       [3.95204395e-01, 5.98775089e-01, 6.02050917e-03],\n",
       "       [1.02553954e-02, 9.71988440e-01, 1.77561305e-02],\n",
       "       [1.68434822e-03, 4.54677083e-03, 9.93768871e-01],\n",
       "       [5.69675677e-03, 8.86195779e-01, 1.08107455e-01],\n",
       "       [1.63205132e-01, 2.02551872e-01, 6.34242952e-01],\n",
       "       [2.51895152e-02, 9.61079895e-01, 1.37305520e-02],\n",
       "       [2.41799350e-03, 9.35761556e-02, 9.04005826e-01],\n",
       "       [8.02401628e-04, 8.80497228e-03, 9.90392685e-01],\n",
       "       [9.19273198e-01, 5.01241460e-02, 3.06026042e-02],\n",
       "       [1.45425811e-01, 7.80847669e-01, 7.37265721e-02],\n",
       "       [1.75698306e-02, 1.19960539e-01, 8.62469614e-01],\n",
       "       [1.80902973e-01, 7.87677884e-01, 3.14190686e-02],\n",
       "       [2.29445606e-01, 7.69490421e-01, 1.06398331e-03],\n",
       "       [1.97446905e-03, 1.27294092e-02, 9.85296071e-01],\n",
       "       [9.43725049e-01, 5.37686981e-03, 5.08980080e-02],\n",
       "       [1.14555173e-01, 7.86031485e-01, 9.94134322e-02],\n",
       "       [4.53832559e-02, 7.59662986e-02, 8.78650486e-01],\n",
       "       [3.71293561e-03, 6.30921006e-01, 3.65366161e-01],\n",
       "       [4.84704884e-04, 1.38811823e-02, 9.85634148e-01],\n",
       "       [2.70868447e-02, 8.40374291e-01, 1.32538795e-01],\n",
       "       [9.91822183e-01, 6.09958079e-03, 2.07817508e-03],\n",
       "       [4.34385007e-03, 9.87961113e-01, 7.69501133e-03],\n",
       "       [1.77238677e-02, 3.99583131e-01, 5.82692981e-01],\n",
       "       [1.05385110e-02, 9.02018905e-01, 8.74425694e-02],\n",
       "       [1.33870944e-01, 8.19808543e-01, 4.63205092e-02],\n",
       "       [1.26580134e-01, 8.16874385e-01, 5.65454438e-02],\n",
       "       [2.34901711e-01, 7.47214854e-01, 1.78834479e-02],\n",
       "       [5.02282321e-01, 4.90656823e-01, 7.06087006e-03],\n",
       "       [4.47097328e-03, 2.57286597e-02, 9.69800353e-01],\n",
       "       [5.39840618e-03, 9.88410950e-01, 6.19065203e-03],\n",
       "       [1.41660616e-01, 4.07011896e-01, 4.51327413e-01],\n",
       "       [8.36233329e-03, 9.46291447e-01, 4.53461148e-02],\n",
       "       [8.74997675e-03, 9.66547668e-01, 2.47023460e-02],\n",
       "       [5.89922220e-02, 4.70225722e-01, 4.70782071e-01],\n",
       "       [7.48337328e-01, 1.77984357e-01, 7.36782998e-02],\n",
       "       [1.16009526e-02, 9.84459639e-01, 3.93944792e-03],\n",
       "       [1.34586647e-01, 6.93757296e-01, 1.71656102e-01],\n",
       "       [5.99124469e-03, 9.65199888e-01, 2.88088676e-02],\n",
       "       [6.08713785e-03, 2.73889154e-01, 7.20023692e-01],\n",
       "       [2.53279835e-01, 6.97211921e-01, 4.95081507e-02],\n",
       "       [3.08638955e-05, 9.79144275e-01, 2.08248701e-02],\n",
       "       [1.93734646e-01, 7.80686915e-01, 2.55784709e-02],\n",
       "       [2.73033488e-03, 4.00322706e-01, 5.96946955e-01],\n",
       "       [1.13829286e-04, 8.13574623e-03, 9.91750419e-01]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6928571428571428"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test)\n",
    "np.mean(np.argmax(y_test, axis=1) == np.argmax(y_test_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    filters1, filters2 = filters\n",
    "\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = layers.Conv2D(filters1, (1, 1),\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2a')(input_tensor)\n",
    "    x = layers.BatchNormalization(axis=3, name=bn_name_base + '2a')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters2, kernel_size,\n",
    "                      padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2b')(x)\n",
    "    x = layers.BatchNormalization(axis=3, name=bn_name_base + '2b')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(input_tensor,\n",
    "               kernel_size,\n",
    "               filters,\n",
    "               stage,\n",
    "               block,\n",
    "               strides=(2, 2)):\n",
    "    \"\"\"A block that has a conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "        strides: Strides for the first conv layer in the block.\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    Note that from stage 3,\n",
    "    the first conv layer at main path is with strides=(2, 2)\n",
    "    And the shortcut should have strides=(2, 2) as well\n",
    "    \"\"\"\n",
    "    filters1, filters2 = filters\n",
    "    \n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = layers.Conv2D(filters1, (1, 1), strides=strides,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2a')(input_tensor)\n",
    "    x = layers.BatchNormalization(axis=3, name=bn_name_base + '2a')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters2, kernel_size, padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2b')(x)\n",
    "    x = layers.BatchNormalization(axis=3, name=bn_name_base + '2b')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    shortcut = layers.Conv2D(filters2, (1, 1), strides=strides,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             name=conv_name_base + '1')(input_tensor)\n",
    "    shortcut = layers.BatchNormalization(\n",
    "        axis=3, name=bn_name_base + '1')(shortcut)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 100, 100, 30) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 106, 106, 30) 0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 50, 50, 64)   94144       conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 50, 50, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 50, 50, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 24, 24, 64)   0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 24, 24, 64)   4160        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 24, 24, 64)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 24, 24, 64)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 24, 24, 64)   36928       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 24, 24, 64)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 24, 24, 64)   4160        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 24, 24, 64)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 24, 24, 64)   256         res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 24, 24, 64)   0           activation_13[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 24, 24, 64)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 24, 24, 64)   4160        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 24, 24, 64)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 24, 24, 64)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 24, 24, 64)   36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 24, 24, 64)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 24, 24, 64)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 24, 24, 64)   0           activation_16[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 24, 24, 64)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 24, 24, 64)   4160        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 24, 24, 64)   256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 24, 24, 64)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 24, 24, 64)   36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 24, 24, 64)   256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 24, 24, 64)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 24, 24, 64)   0           activation_19[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 24, 24, 64)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 64)           0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "fc1000 (Dense)                  (None, 3)            195         avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 223,811\n",
      "Trainable params: 222,787\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "IMSIZE = 100\n",
    "input_layer = layers.Input([IMSIZE, IMSIZE, 30])\n",
    "\n",
    "x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(input_layer)\n",
    "x = layers.Conv2D(64, (7, 7),\n",
    "                  strides=(2, 2),\n",
    "                  padding='valid',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  name='conv1')(x)\n",
    "x = layers.BatchNormalization(axis=3, name='bn_conv1')(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "x = conv_block(x, 3, [64, 64], stage=2, block='a', strides=(1, 1))\n",
    "x = identity_block(x, 3, [64, 64], stage=2, block='b')\n",
    "x = identity_block(x, 3, [64, 64], stage=2, block='c')\n",
    "\n",
    "x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "output_layer = layers.Dense(3, activation='softmax', name='fc1000')(x)\n",
    "\n",
    "model = Model(input_layer,output_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 896 samples, validate on 224 samples\n",
      "Epoch 1/25\n",
      "896/896 [==============================] - 36s 40ms/step - loss: 1.2124 - accuracy: 0.4330 - val_loss: 1.3171 - val_accuracy: 0.3304\n",
      "Epoch 2/25\n",
      "896/896 [==============================] - 32s 36ms/step - loss: 1.0159 - accuracy: 0.4565 - val_loss: 1.4262 - val_accuracy: 0.3884\n",
      "Epoch 3/25\n",
      "896/896 [==============================] - 30s 33ms/step - loss: 0.9616 - accuracy: 0.5268 - val_loss: 1.2381 - val_accuracy: 0.3795\n",
      "Epoch 4/25\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 0.9086 - accuracy: 0.5558 - val_loss: 1.2992 - val_accuracy: 0.3616\n",
      "Epoch 5/25\n",
      "896/896 [==============================] - 31s 34ms/step - loss: 0.8664 - accuracy: 0.5982 - val_loss: 1.2225 - val_accuracy: 0.3839\n",
      "Epoch 6/25\n",
      "896/896 [==============================] - 31s 35ms/step - loss: 0.8586 - accuracy: 0.5893 - val_loss: 1.4035 - val_accuracy: 0.3571\n",
      "Epoch 7/25\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 0.7931 - accuracy: 0.6574 - val_loss: 1.1374 - val_accuracy: 0.3750\n",
      "Epoch 8/25\n",
      "896/896 [==============================] - 38s 42ms/step - loss: 0.7635 - accuracy: 0.6696 - val_loss: 1.2874 - val_accuracy: 0.4152\n",
      "Epoch 9/25\n",
      "896/896 [==============================] - 31s 34ms/step - loss: 0.6999 - accuracy: 0.7076 - val_loss: 1.3729 - val_accuracy: 0.3795\n",
      "Epoch 10/25\n",
      "896/896 [==============================] - 31s 34ms/step - loss: 0.6860 - accuracy: 0.7143 - val_loss: 1.1100 - val_accuracy: 0.4420\n",
      "Epoch 11/25\n",
      "896/896 [==============================] - 31s 35ms/step - loss: 0.6266 - accuracy: 0.7522 - val_loss: 1.2418 - val_accuracy: 0.4107\n",
      "Epoch 12/25\n",
      "896/896 [==============================] - 31s 34ms/step - loss: 0.5677 - accuracy: 0.8013 - val_loss: 2.5470 - val_accuracy: 0.3259\n",
      "Epoch 13/25\n",
      "896/896 [==============================] - 30s 33ms/step - loss: 0.5619 - accuracy: 0.7902 - val_loss: 1.3102 - val_accuracy: 0.4152\n",
      "Epoch 14/25\n",
      "896/896 [==============================] - 30s 34ms/step - loss: 0.4849 - accuracy: 0.8393 - val_loss: 4.9602 - val_accuracy: 0.2946\n",
      "Epoch 15/25\n",
      "896/896 [==============================] - 30s 34ms/step - loss: 0.4343 - accuracy: 0.8516 - val_loss: 2.9558 - val_accuracy: 0.3036\n",
      "Epoch 16/25\n",
      "896/896 [==============================] - 34s 37ms/step - loss: 0.4079 - accuracy: 0.8661 - val_loss: 3.0506 - val_accuracy: 0.3795\n",
      "Epoch 17/25\n",
      "896/896 [==============================] - 32s 36ms/step - loss: 0.4979 - accuracy: 0.7980 - val_loss: 5.5586 - val_accuracy: 0.3036\n",
      "Epoch 18/25\n",
      "896/896 [==============================] - 32s 36ms/step - loss: 0.3768 - accuracy: 0.8750 - val_loss: 14.3235 - val_accuracy: 0.2812\n",
      "Epoch 19/25\n",
      "896/896 [==============================] - 32s 36ms/step - loss: 0.3040 - accuracy: 0.9185 - val_loss: 3.8502 - val_accuracy: 0.3661\n",
      "Epoch 20/25\n",
      "896/896 [==============================] - 29s 33ms/step - loss: 0.2396 - accuracy: 0.9509 - val_loss: 4.5813 - val_accuracy: 0.3884\n",
      "Epoch 21/25\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 0.2063 - accuracy: 0.9609 - val_loss: 3.5050 - val_accuracy: 0.3304\n",
      "Epoch 22/25\n",
      "896/896 [==============================] - 31s 34ms/step - loss: 0.2234 - accuracy: 0.9565 - val_loss: 2.5851 - val_accuracy: 0.3661\n",
      "Epoch 23/25\n",
      "896/896 [==============================] - 28s 32ms/step - loss: 0.1840 - accuracy: 0.9721 - val_loss: 4.5093 - val_accuracy: 0.4196\n",
      "Epoch 24/25\n",
      "896/896 [==============================] - 29s 33ms/step - loss: 0.1400 - accuracy: 0.9844 - val_loss: 3.0539 - val_accuracy: 0.4554\n",
      "Epoch 25/25\n",
      "896/896 [==============================] - 29s 32ms/step - loss: 0.1015 - accuracy: 0.9900 - val_loss: 4.9025 - val_accuracy: 0.4286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x268ecc9e8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=50, \n",
    "          epochs=25, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.425"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test)\n",
    "np.mean(np.argmax(y_test, axis=1) == np.argmax(y_test_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    filters1, filters2 = filters\n",
    "\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = layers.Conv2D(filters1, (1, 1),\n",
    "                      kernel_initializer='he_normal',\n",
    "                      activation='relu',\n",
    "                      name=conv_name_base + '2a')(input_tensor)\n",
    "\n",
    "    x = layers.Conv2D(filters2, kernel_size,\n",
    "                      padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2b')(x)\n",
    "    x = layers.BatchNormalization(axis=3, name=bn_name_base + '2a')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(input_tensor,\n",
    "               kernel_size,\n",
    "               filters,\n",
    "               stage,\n",
    "               block,\n",
    "               strides=(2, 2)):\n",
    "    \"\"\"A block that has a conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "        strides: Strides for the first conv layer in the block.\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    Note that from stage 3,\n",
    "    the first conv layer at main path is with strides=(2, 2)\n",
    "    And the shortcut should have strides=(2, 2) as well\n",
    "    \"\"\"\n",
    "    filters1, filters2 = filters\n",
    "    \n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = layers.Conv2D(filters1, (1, 1), strides=strides,\n",
    "                      kernel_initializer='he_normal', activation='relu',\n",
    "                      name=conv_name_base + '2a')(input_tensor)\n",
    "\n",
    "    x = layers.Conv2D(filters2, kernel_size, padding='same',\n",
    "                      kernel_initializer='he_normal', activation='relu',\n",
    "                      name=conv_name_base + '2b')(x)\n",
    "    x = layers.BatchNormalization(axis=3, name=bn_name_base + '2a')(x)\n",
    "\n",
    "    shortcut = layers.Conv2D(filters2, (1, 1), strides=strides,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             name=conv_name_base + '1')(input_tensor)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 100, 100, 30) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 106, 106, 30) 0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 50, 50, 64)   94144       conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 50, 50, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 24, 24, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 24, 24, 64)   4160        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 24, 24, 128)  73856       res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 24, 24, 128)  512         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 24, 24, 128)  8320        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 24, 24, 128)  0           bn2a_branch2a[0][0]              \n",
      "                                                                 res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 24, 24, 128)  0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 24, 24, 64)   8256        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 24, 24, 128)  73856       res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 24, 24, 128)  512         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 24, 24, 128)  0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 24, 24, 128)  0           activation_27[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 24, 24, 64)   8256        add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 24, 24, 128)  73856       res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 24, 24, 128)  512         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 24, 24, 128)  0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 24, 24, 128)  0           activation_28[0][0]              \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 128)          0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fc1000 (Dense)                  (None, 3)            387         avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 346,883\n",
      "Trainable params: 345,987\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "IMSIZE = 100\n",
    "input_layer = layers.Input([IMSIZE, IMSIZE, 30])\n",
    "\n",
    "x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(input_layer)\n",
    "x = layers.Conv2D(64, (7, 7),\n",
    "                  strides=(2, 2),\n",
    "                  padding='valid',\n",
    "                  activation='relu',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  name='conv1')(x)\n",
    "x = layers.BatchNormalization(axis=3, name='bn_conv1')(x)\n",
    "x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "x = conv_block(x, 3, [64, 128], stage=2, block='a', strides=(1, 1))\n",
    "x = identity_block(x, 3, [64, 128], stage=2, block='b')\n",
    "x = identity_block(x, 3, [64, 128], stage=2, block='c')\n",
    "\n",
    "x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "output_layer = layers.Dense(3, activation='softmax', name='fc1000')(x)\n",
    "\n",
    "model = Model(input_layer,output_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 896 samples, validate on 224 samples\n",
      "Epoch 1/25\n",
      "896/896 [==============================] - 31s 35ms/step - loss: 1.3336 - accuracy: 0.4040 - val_loss: 3.5482 - val_accuracy: 0.3304\n",
      "Epoch 2/25\n",
      "896/896 [==============================] - 32s 36ms/step - loss: 1.0222 - accuracy: 0.4587 - val_loss: 1.5987 - val_accuracy: 0.3304\n",
      "Epoch 3/25\n",
      "896/896 [==============================] - 34s 38ms/step - loss: 0.9808 - accuracy: 0.5167 - val_loss: 1.2268 - val_accuracy: 0.3929\n",
      "Epoch 4/25\n",
      "896/896 [==============================] - 35s 39ms/step - loss: 0.9508 - accuracy: 0.5424 - val_loss: 2.0290 - val_accuracy: 0.3304\n",
      "Epoch 5/25\n",
      "896/896 [==============================] - 35s 39ms/step - loss: 0.9353 - accuracy: 0.5324 - val_loss: 1.6030 - val_accuracy: 0.3348\n",
      "Epoch 6/25\n",
      "896/896 [==============================] - 35s 39ms/step - loss: 0.8659 - accuracy: 0.5792 - val_loss: 1.5233 - val_accuracy: 0.3348\n",
      "Epoch 7/25\n",
      "896/896 [==============================] - 36s 40ms/step - loss: 0.7989 - accuracy: 0.6239 - val_loss: 1.4811 - val_accuracy: 0.3348\n",
      "Epoch 8/25\n",
      "896/896 [==============================] - 35s 39ms/step - loss: 0.7506 - accuracy: 0.6551 - val_loss: 2.9647 - val_accuracy: 0.3304\n",
      "Epoch 9/25\n",
      "896/896 [==============================] - 35s 39ms/step - loss: 0.6749 - accuracy: 0.7076 - val_loss: 2.0271 - val_accuracy: 0.3304\n",
      "Epoch 10/25\n",
      "896/896 [==============================] - 35s 39ms/step - loss: 0.5839 - accuracy: 0.7656 - val_loss: 1.2369 - val_accuracy: 0.4107\n",
      "Epoch 11/25\n",
      "896/896 [==============================] - 34s 38ms/step - loss: 0.4594 - accuracy: 0.8270 - val_loss: 1.1641 - val_accuracy: 0.4911\n",
      "Epoch 12/25\n",
      "896/896 [==============================] - 34s 38ms/step - loss: 0.4304 - accuracy: 0.8326 - val_loss: 1.9044 - val_accuracy: 0.3661\n",
      "Epoch 13/25\n",
      "896/896 [==============================] - 34s 38ms/step - loss: 0.3567 - accuracy: 0.8828 - val_loss: 2.3094 - val_accuracy: 0.3527\n",
      "Epoch 14/25\n",
      "896/896 [==============================] - 34s 38ms/step - loss: 0.2301 - accuracy: 0.9643 - val_loss: 1.1645 - val_accuracy: 0.4420\n",
      "Epoch 15/25\n",
      "896/896 [==============================] - 34s 37ms/step - loss: 0.2026 - accuracy: 0.9475 - val_loss: 3.7123 - val_accuracy: 0.3304\n",
      "Epoch 16/25\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 0.1456 - accuracy: 0.9799 - val_loss: 1.9371 - val_accuracy: 0.3661\n",
      "Epoch 17/25\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 0.1107 - accuracy: 0.9911 - val_loss: 3.1174 - val_accuracy: 0.3393\n",
      "Epoch 18/25\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 0.0802 - accuracy: 0.9944 - val_loss: 1.0409 - val_accuracy: 0.5625\n",
      "Epoch 19/25\n",
      "896/896 [==============================] - 113s 126ms/step - loss: 0.0546 - accuracy: 0.9967 - val_loss: 4.2168 - val_accuracy: 0.3304\n",
      "Epoch 20/25\n",
      "896/896 [==============================] - 32s 36ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 3.2465 - val_accuracy: 0.3348\n",
      "Epoch 21/25\n",
      "896/896 [==============================] - 41s 46ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 1.0826 - val_accuracy: 0.5580\n",
      "Epoch 22/25\n",
      "896/896 [==============================] - 35s 40ms/step - loss: 0.0325 - accuracy: 0.9989 - val_loss: 1.7136 - val_accuracy: 0.4554\n",
      "Epoch 23/25\n",
      "896/896 [==============================] - 36s 40ms/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 2.1231 - val_accuracy: 0.4464\n",
      "Epoch 24/25\n",
      "896/896 [==============================] - 36s 40ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 1.0905 - val_accuracy: 0.5893\n",
      "Epoch 25/25\n",
      "896/896 [==============================] - 37s 41ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 1.2599 - val_accuracy: 0.5804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x26b3ebfd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=50, \n",
    "          epochs=25, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5642857142857143"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test)\n",
    "np.mean(np.argmax(y_test, axis=1) == np.argmax(y_test_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([ 80,  99, 101]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.argmax(y_test, axis=1), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 896 samples, validate on 224 samples\n",
      "Epoch 1/40\n",
      "896/896 [==============================] - 32s 35ms/step - loss: 0.0502 - accuracy: 0.9955 - val_loss: 2.0125 - val_accuracy: 0.4375\n",
      "Epoch 2/40\n",
      "896/896 [==============================] - 32s 36ms/step - loss: 0.0668 - accuracy: 0.9844 - val_loss: 5.0231 - val_accuracy: 0.3750\n",
      "Epoch 3/40\n",
      "896/896 [==============================] - 35s 39ms/step - loss: 0.0689 - accuracy: 0.9777 - val_loss: 8.2067 - val_accuracy: 0.3304\n",
      "Epoch 4/40\n",
      "896/896 [==============================] - 36s 40ms/step - loss: 0.0665 - accuracy: 0.9877 - val_loss: 3.0719 - val_accuracy: 0.4107\n",
      "Epoch 5/40\n",
      "896/896 [==============================] - 36s 40ms/step - loss: 0.0505 - accuracy: 0.9911 - val_loss: 1.8194 - val_accuracy: 0.4732\n",
      "Epoch 6/40\n",
      "896/896 [==============================] - 37s 41ms/step - loss: 0.0280 - accuracy: 0.9978 - val_loss: 3.1540 - val_accuracy: 0.4107\n",
      "Epoch 7/40\n",
      "896/896 [==============================] - 37s 41ms/step - loss: 0.0219 - accuracy: 0.9967 - val_loss: 3.6193 - val_accuracy: 0.4821\n",
      "Epoch 8/40\n",
      "896/896 [==============================] - 35s 40ms/step - loss: 0.0317 - accuracy: 0.9922 - val_loss: 8.8146 - val_accuracy: 0.3348\n",
      "Epoch 9/40\n",
      "896/896 [==============================] - 35s 39ms/step - loss: 0.0275 - accuracy: 0.9955 - val_loss: 1.5130 - val_accuracy: 0.5759\n",
      "Epoch 10/40\n",
      "896/896 [==============================] - 35s 40ms/step - loss: 0.0551 - accuracy: 0.9833 - val_loss: 2.5885 - val_accuracy: 0.4554\n",
      "Epoch 11/40\n",
      "896/896 [==============================] - 35s 39ms/step - loss: 0.0438 - accuracy: 0.9888 - val_loss: 4.8562 - val_accuracy: 0.4196\n",
      "Epoch 12/40\n",
      "896/896 [==============================] - 35s 39ms/step - loss: 0.0290 - accuracy: 0.9978 - val_loss: 3.4556 - val_accuracy: 0.4866\n",
      "Epoch 13/40\n",
      "896/896 [==============================] - 35s 39ms/step - loss: 0.0233 - accuracy: 0.9967 - val_loss: 1.4092 - val_accuracy: 0.6250\n",
      "Epoch 14/40\n",
      "896/896 [==============================] - 34s 38ms/step - loss: 0.0191 - accuracy: 0.9967 - val_loss: 4.1403 - val_accuracy: 0.4688\n",
      "Epoch 15/40\n",
      "896/896 [==============================] - 34s 38ms/step - loss: 0.0145 - accuracy: 0.9989 - val_loss: 4.7496 - val_accuracy: 0.4062\n",
      "Epoch 16/40\n",
      "896/896 [==============================] - 35s 39ms/step - loss: 0.0139 - accuracy: 0.9967 - val_loss: 1.9009 - val_accuracy: 0.5670\n",
      "Epoch 17/40\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 2.4660 - val_accuracy: 0.4911\n",
      "Epoch 18/40\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3832 - val_accuracy: 0.6161\n",
      "Epoch 19/40\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.2805 - val_accuracy: 0.6384\n",
      "Epoch 20/40\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.0922 - val_accuracy: 0.6920\n",
      "Epoch 21/40\n",
      "896/896 [==============================] - 34s 37ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.2731 - val_accuracy: 0.6607\n",
      "Epoch 22/40\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.1345 - val_accuracy: 0.6920\n",
      "Epoch 23/40\n",
      "896/896 [==============================] - 34s 38ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.1434 - val_accuracy: 0.6741\n",
      "Epoch 24/40\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.3880 - val_accuracy: 0.6384\n",
      "Epoch 25/40\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 9.9674e-04 - accuracy: 1.0000 - val_loss: 1.2693 - val_accuracy: 0.6518\n",
      "Epoch 26/40\n",
      "896/896 [==============================] - 34s 38ms/step - loss: 8.2888e-04 - accuracy: 1.0000 - val_loss: 1.0949 - val_accuracy: 0.6964\n",
      "Epoch 27/40\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.0568 - val_accuracy: 0.6920\n",
      "Epoch 28/40\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 7.0151e-04 - accuracy: 1.0000 - val_loss: 1.0602 - val_accuracy: 0.6920\n",
      "Epoch 29/40\n",
      "896/896 [==============================] - 34s 38ms/step - loss: 6.5902e-04 - accuracy: 1.0000 - val_loss: 1.0648 - val_accuracy: 0.7009\n",
      "Epoch 30/40\n",
      "896/896 [==============================] - 39s 43ms/step - loss: 9.7459e-04 - accuracy: 1.0000 - val_loss: 1.2871 - val_accuracy: 0.6607\n",
      "Epoch 31/40\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 6.0180e-04 - accuracy: 1.0000 - val_loss: 1.2914 - val_accuracy: 0.6696\n",
      "Epoch 32/40\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 7.4298e-04 - accuracy: 1.0000 - val_loss: 1.2528 - val_accuracy: 0.6696\n",
      "Epoch 33/40\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 6.9315e-04 - accuracy: 1.0000 - val_loss: 1.1482 - val_accuracy: 0.7054\n",
      "Epoch 34/40\n",
      "896/896 [==============================] - 38s 43ms/step - loss: 7.5087e-04 - accuracy: 1.0000 - val_loss: 1.2350 - val_accuracy: 0.6920\n",
      "Epoch 35/40\n",
      "896/896 [==============================] - 34s 38ms/step - loss: 6.8825e-04 - accuracy: 1.0000 - val_loss: 1.2113 - val_accuracy: 0.7009\n",
      "Epoch 36/40\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 5.4672e-04 - accuracy: 1.0000 - val_loss: 1.1969 - val_accuracy: 0.7054\n",
      "Epoch 37/40\n",
      "896/896 [==============================] - 33s 36ms/step - loss: 6.6293e-04 - accuracy: 1.0000 - val_loss: 1.1288 - val_accuracy: 0.7098\n",
      "Epoch 38/40\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 5.7518e-04 - accuracy: 1.0000 - val_loss: 1.0951 - val_accuracy: 0.7143\n",
      "Epoch 39/40\n",
      "896/896 [==============================] - 33s 37ms/step - loss: 4.3975e-04 - accuracy: 1.0000 - val_loss: 1.0901 - val_accuracy: 0.7098\n",
      "Epoch 40/40\n",
      "896/896 [==============================] - 34s 38ms/step - loss: 4.5426e-04 - accuracy: 1.0000 - val_loss: 1.1337 - val_accuracy: 0.7188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x26ce2dfd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=50, \n",
    "          epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6535714285714286"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test)\n",
    "np.mean(np.argmax(y_test, axis=1) == np.argmax(y_test_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.33089386e-02, 9.78519261e-01, 8.17178376e-03],\n",
       "       [5.87763369e-01, 3.34178835e-01, 7.80578405e-02],\n",
       "       [4.28377498e-05, 7.23131076e-02, 9.27644014e-01],\n",
       "       [9.93621051e-01, 5.88057376e-03, 4.98277601e-04],\n",
       "       [6.51015714e-02, 7.86492407e-01, 1.48406044e-01],\n",
       "       [2.78795838e-01, 1.05335392e-01, 6.15868747e-01],\n",
       "       [3.47530276e-01, 1.45680517e-01, 5.06789148e-01],\n",
       "       [2.75418401e-01, 7.23439455e-01, 1.14220742e-03],\n",
       "       [9.94773924e-01, 4.66954429e-03, 5.56504296e-04],\n",
       "       [2.97659775e-04, 9.48392153e-01, 5.13101518e-02],\n",
       "       [4.55379905e-03, 9.95314956e-01, 1.31253575e-04],\n",
       "       [7.02044636e-04, 8.78735408e-02, 9.11424398e-01],\n",
       "       [9.88217056e-01, 1.05651850e-02, 1.21778331e-03],\n",
       "       [2.10188683e-02, 9.78770673e-01, 2.10445054e-04],\n",
       "       [8.73447120e-01, 3.33010466e-05, 1.26519561e-01],\n",
       "       [2.04733922e-04, 1.13005197e-04, 9.99682307e-01],\n",
       "       [1.89436381e-04, 1.11322394e-02, 9.88678277e-01],\n",
       "       [3.61694751e-04, 9.99360621e-01, 2.77661835e-04],\n",
       "       [9.98936117e-01, 9.39438352e-04, 1.24476661e-04],\n",
       "       [3.68456458e-05, 9.99118745e-01, 8.44408467e-04],\n",
       "       [1.04174323e-05, 9.99896765e-01, 9.29007656e-05],\n",
       "       [8.73707056e-01, 1.13314368e-01, 1.29786395e-02],\n",
       "       [1.11829964e-02, 8.34389552e-02, 9.05378044e-01],\n",
       "       [1.08294383e-01, 8.91688406e-01, 1.72456603e-05],\n",
       "       [9.32718933e-01, 6.62241876e-02, 1.05693517e-03],\n",
       "       [4.61140200e-02, 9.51530397e-01, 2.35561840e-03],\n",
       "       [9.50905323e-01, 4.49808799e-02, 4.11384599e-03],\n",
       "       [9.73935187e-01, 1.96348828e-05, 2.60451939e-02],\n",
       "       [8.55949938e-01, 2.67253332e-02, 1.17324673e-01],\n",
       "       [1.12401714e-04, 9.86015856e-01, 1.38717536e-02],\n",
       "       [5.97543120e-02, 9.40181017e-01, 6.46789049e-05],\n",
       "       [1.82640890e-03, 5.94127297e-01, 4.04046327e-01],\n",
       "       [1.86171514e-04, 1.42114842e-03, 9.98392642e-01],\n",
       "       [8.26833665e-01, 1.73161060e-01, 5.19543664e-06],\n",
       "       [2.98071632e-06, 1.38307452e-01, 8.61689568e-01],\n",
       "       [6.50070497e-10, 1.18322703e-07, 9.99999881e-01],\n",
       "       [3.93127859e-01, 5.43433905e-01, 6.34382889e-02],\n",
       "       [1.02667222e-07, 6.28471468e-03, 9.93715227e-01],\n",
       "       [2.16700113e-03, 5.25177568e-02, 9.45315182e-01],\n",
       "       [9.51606333e-01, 4.58063744e-03, 4.38130423e-02],\n",
       "       [9.94372964e-01, 5.19641535e-03, 4.30664077e-04],\n",
       "       [2.68272489e-01, 1.63774565e-01, 5.67952991e-01],\n",
       "       [1.58659958e-07, 9.93747473e-01, 6.25243317e-03],\n",
       "       [9.87170279e-01, 1.05093839e-02, 2.32031033e-03],\n",
       "       [7.03495753e-05, 7.18407571e-01, 2.81522125e-01],\n",
       "       [1.13731958e-01, 6.23127401e-01, 2.63140649e-01],\n",
       "       [2.73262238e-04, 2.53039487e-02, 9.74422812e-01],\n",
       "       [9.56612930e-04, 9.86736834e-01, 1.23065514e-02],\n",
       "       [9.12674877e-05, 7.25082075e-03, 9.92657959e-01],\n",
       "       [2.41621620e-07, 3.11103463e-03, 9.96888697e-01],\n",
       "       [9.68130171e-01, 2.81540696e-02, 3.71572492e-03],\n",
       "       [8.94612491e-01, 1.05353877e-01, 3.36111152e-05],\n",
       "       [6.06370857e-03, 9.93805885e-01, 1.30458371e-04],\n",
       "       [3.03700328e-01, 6.93946064e-01, 2.35357555e-03],\n",
       "       [9.89864647e-01, 5.99878980e-03, 4.13649110e-03],\n",
       "       [8.50235857e-03, 9.91132796e-01, 3.64849868e-04],\n",
       "       [9.46085513e-01, 5.21724410e-02, 1.74201024e-03],\n",
       "       [4.75895293e-02, 9.52106118e-01, 3.04408168e-04],\n",
       "       [9.99984980e-01, 6.92120830e-06, 8.05624677e-06],\n",
       "       [4.04059276e-04, 2.03687698e-02, 9.79227126e-01],\n",
       "       [2.15424348e-06, 9.70799327e-01, 2.91984677e-02],\n",
       "       [4.39658731e-01, 5.16821086e-01, 4.35201488e-02],\n",
       "       [3.03971339e-02, 9.68668163e-01, 9.34786804e-04],\n",
       "       [1.01100901e-04, 2.09443551e-02, 9.78954613e-01],\n",
       "       [1.34968896e-05, 6.48334682e-01, 3.51651818e-01],\n",
       "       [1.22143421e-02, 9.69936252e-01, 1.78494155e-02],\n",
       "       [4.17403389e-05, 4.77785617e-03, 9.95180368e-01],\n",
       "       [5.03167808e-01, 1.45137221e-01, 3.51695061e-01],\n",
       "       [6.56433185e-05, 8.39384437e-01, 1.60549954e-01],\n",
       "       [2.00763054e-04, 1.01121649e-01, 8.98677588e-01],\n",
       "       [1.14474510e-07, 5.33151925e-01, 4.66847986e-01],\n",
       "       [1.64780188e-02, 4.98173565e-01, 4.85348403e-01],\n",
       "       [1.07559972e-05, 6.22999971e-04, 9.99366224e-01],\n",
       "       [1.31900506e-02, 2.63051093e-01, 7.23758817e-01],\n",
       "       [8.15974891e-01, 1.82898834e-01, 1.12619018e-03],\n",
       "       [1.84195014e-05, 2.93210745e-01, 7.06770837e-01],\n",
       "       [5.78778207e-01, 3.74470511e-03, 4.17477131e-01],\n",
       "       [9.61244881e-01, 3.75146605e-02, 1.24048395e-03],\n",
       "       [2.42656624e-05, 2.49171304e-03, 9.97483969e-01],\n",
       "       [4.23905224e-01, 1.67591631e-01, 4.08503175e-01],\n",
       "       [1.31779416e-05, 9.38783407e-01, 6.12034351e-02],\n",
       "       [8.13079299e-04, 6.95726335e-01, 3.03460598e-01],\n",
       "       [2.39051878e-04, 9.99753058e-01, 7.91228376e-06],\n",
       "       [8.55884142e-03, 9.46384132e-01, 4.50570546e-02],\n",
       "       [1.03394734e-04, 1.15699153e-02, 9.88326728e-01],\n",
       "       [9.68563676e-01, 5.35383588e-03, 2.60824934e-02],\n",
       "       [1.23045463e-02, 6.04129769e-02, 9.27282453e-01],\n",
       "       [7.40986168e-01, 1.19605458e-04, 2.58894205e-01],\n",
       "       [3.58839631e-01, 6.20897114e-01, 2.02632379e-02],\n",
       "       [1.39680514e-02, 1.88040998e-04, 9.85843897e-01],\n",
       "       [9.92339134e-01, 5.85390721e-03, 1.80702005e-03],\n",
       "       [3.64472980e-05, 8.88266802e-01, 1.11696713e-01],\n",
       "       [7.76053190e-01, 5.62459791e-05, 2.23890573e-01],\n",
       "       [5.44071849e-03, 9.54270482e-01, 4.02887836e-02],\n",
       "       [1.72776214e-04, 9.56505179e-01, 4.33221087e-02],\n",
       "       [8.61847401e-01, 1.15393959e-01, 2.27587018e-02],\n",
       "       [1.55184616e-03, 8.59381974e-01, 1.39066115e-01],\n",
       "       [9.78720188e-01, 7.18813099e-05, 2.12079324e-02],\n",
       "       [3.34781259e-02, 9.41324830e-01, 2.51970552e-02],\n",
       "       [9.97799337e-01, 2.19224836e-03, 8.42295776e-06],\n",
       "       [8.99163783e-02, 9.09786761e-01, 2.96886225e-04],\n",
       "       [1.34716138e-09, 9.99923825e-01, 7.62164345e-05],\n",
       "       [9.69468474e-01, 2.97320224e-02, 7.99525820e-04],\n",
       "       [7.30203688e-02, 9.26826656e-01, 1.52898239e-04],\n",
       "       [9.95977998e-01, 4.00619023e-03, 1.58361254e-05],\n",
       "       [4.55938429e-01, 4.97085482e-01, 4.69760820e-02],\n",
       "       [4.26124334e-01, 1.91381633e-01, 3.82494032e-01],\n",
       "       [4.81937568e-05, 3.60019127e-04, 9.99591768e-01],\n",
       "       [2.28203163e-01, 3.54596972e-01, 4.17199880e-01],\n",
       "       [3.56422691e-03, 4.14604068e-01, 5.81831634e-01],\n",
       "       [1.30426441e-03, 8.89361203e-01, 1.09334499e-01],\n",
       "       [3.69608611e-01, 6.29634440e-01, 7.56986090e-04],\n",
       "       [1.72250066e-02, 9.21556413e-01, 6.12186454e-02],\n",
       "       [3.97607148e-01, 5.94529212e-01, 7.86362682e-03],\n",
       "       [1.32756755e-01, 1.76173989e-02, 8.49625826e-01],\n",
       "       [9.68740523e-01, 3.07965428e-02, 4.62964410e-04],\n",
       "       [3.78788263e-03, 3.05110574e-01, 6.91101551e-01],\n",
       "       [7.13725967e-05, 2.09918525e-03, 9.97829378e-01],\n",
       "       [4.13272381e-01, 5.44591367e-01, 4.21362706e-02],\n",
       "       [4.08548076e-05, 8.34415434e-04, 9.99124706e-01],\n",
       "       [2.91991144e-01, 7.07291543e-01, 7.17276824e-04],\n",
       "       [9.82020199e-01, 1.64953470e-02, 1.48447219e-03],\n",
       "       [7.87061393e-01, 2.12503701e-01, 4.34865098e-04],\n",
       "       [9.96833622e-01, 1.69326563e-03, 1.47310994e-03],\n",
       "       [9.60161328e-01, 5.18402038e-03, 3.46546285e-02],\n",
       "       [7.63340771e-01, 2.36635625e-01, 2.36251690e-05],\n",
       "       [1.70964577e-05, 9.99982357e-01, 6.42720181e-07],\n",
       "       [7.79640377e-01, 3.25072976e-03, 2.17108756e-01],\n",
       "       [3.40487622e-03, 9.74860132e-01, 2.17349455e-02],\n",
       "       [2.02522110e-02, 1.61009992e-03, 9.78137732e-01],\n",
       "       [3.65226839e-07, 1.01540227e-05, 9.99989510e-01],\n",
       "       [2.30898717e-04, 7.97133334e-03, 9.91797805e-01],\n",
       "       [1.44897588e-02, 1.55221626e-01, 8.30288589e-01],\n",
       "       [1.46690011e-01, 5.82885323e-03, 8.47481132e-01],\n",
       "       [1.62394372e-05, 6.58262614e-03, 9.93401110e-01],\n",
       "       [1.01673894e-01, 9.56505071e-03, 8.88761103e-01],\n",
       "       [1.33431668e-03, 4.78634640e-04, 9.98187006e-01],\n",
       "       [4.32007223e-01, 5.24396062e-01, 4.35967483e-02],\n",
       "       [5.45931160e-01, 2.51150370e-01, 2.02918470e-01],\n",
       "       [3.40592487e-05, 2.86430033e-04, 9.99679565e-01],\n",
       "       [5.46667986e-02, 1.06597180e-02, 9.34673429e-01],\n",
       "       [9.48348582e-01, 4.63871025e-02, 5.26428362e-03],\n",
       "       [1.97928414e-01, 5.76561511e-01, 2.25510091e-01],\n",
       "       [6.35181144e-02, 9.36342478e-01, 1.39441036e-04],\n",
       "       [9.87629414e-01, 6.50217105e-03, 5.86846890e-03],\n",
       "       [1.72508342e-04, 2.58513421e-01, 7.41314113e-01],\n",
       "       [9.73930776e-01, 5.61396452e-03, 2.04552524e-02],\n",
       "       [5.17840846e-04, 1.16168465e-02, 9.87865269e-01],\n",
       "       [4.32229979e-04, 9.58337605e-01, 4.12301868e-02],\n",
       "       [3.60841490e-02, 9.57036853e-01, 6.87908567e-03],\n",
       "       [1.12319186e-01, 8.07239294e-01, 8.04414675e-02],\n",
       "       [1.35075018e-01, 7.09772468e-01, 1.55152455e-01],\n",
       "       [9.86602545e-01, 1.33229392e-02, 7.45005018e-05],\n",
       "       [3.05610686e-03, 2.00807210e-02, 9.76863146e-01],\n",
       "       [9.79039133e-01, 2.01819632e-02, 7.78923160e-04],\n",
       "       [1.40817865e-04, 2.44532735e-03, 9.97413874e-01],\n",
       "       [2.31724769e-01, 4.73734051e-01, 2.94541150e-01],\n",
       "       [9.98089731e-01, 1.18783908e-03, 7.22423720e-04],\n",
       "       [6.54446930e-02, 5.72140813e-01, 3.62414509e-01],\n",
       "       [8.87826443e-01, 1.10719100e-01, 1.45446265e-03],\n",
       "       [2.65890296e-04, 1.64310386e-05, 9.99717653e-01],\n",
       "       [2.42555852e-05, 9.53031480e-01, 4.69442643e-02],\n",
       "       [8.89531151e-08, 5.27300035e-05, 9.99947190e-01],\n",
       "       [2.80351105e-05, 4.79262834e-03, 9.95179296e-01],\n",
       "       [2.37711333e-03, 9.97605681e-01, 1.71474021e-05],\n",
       "       [2.31213290e-02, 5.52311957e-01, 4.24566746e-01],\n",
       "       [9.98068631e-01, 1.86835055e-03, 6.30757422e-05],\n",
       "       [9.97221470e-01, 1.31964305e-04, 2.64656381e-03],\n",
       "       [2.95861904e-02, 9.70133424e-01, 2.80327862e-04],\n",
       "       [9.95771825e-01, 4.02594125e-03, 2.02275813e-04],\n",
       "       [4.75035534e-07, 2.52241269e-03, 9.97477114e-01],\n",
       "       [4.44684029e-02, 3.91065702e-02, 9.16424990e-01],\n",
       "       [4.78828923e-08, 9.99978900e-01, 2.11280530e-05],\n",
       "       [3.69510725e-02, 9.62591648e-01, 4.57323418e-04],\n",
       "       [8.75945110e-03, 7.26722658e-01, 2.64517903e-01],\n",
       "       [1.55574526e-04, 9.25219476e-01, 7.46248513e-02],\n",
       "       [9.69968975e-01, 2.71379575e-02, 2.89309234e-03],\n",
       "       [9.41574216e-01, 5.01549197e-03, 5.34103028e-02],\n",
       "       [9.82555509e-01, 5.29381330e-04, 1.69150904e-02],\n",
       "       [9.99765456e-01, 1.13859651e-05, 2.23123658e-04],\n",
       "       [1.19817713e-02, 5.16270339e-01, 4.71747875e-01],\n",
       "       [1.62612960e-01, 8.19315553e-01, 1.80715192e-02],\n",
       "       [3.54694098e-01, 6.44872725e-01, 4.33183275e-04],\n",
       "       [1.14237055e-01, 8.60334158e-01, 2.54288092e-02],\n",
       "       [4.04541343e-02, 9.33993876e-01, 2.55520046e-02],\n",
       "       [2.85663176e-04, 1.44288376e-01, 8.55425954e-01],\n",
       "       [2.50366959e-03, 9.29596066e-01, 6.79003000e-02],\n",
       "       [2.30083519e-04, 4.62908477e-01, 5.36861479e-01],\n",
       "       [9.99390483e-01, 5.74680977e-04, 3.47533314e-05],\n",
       "       [9.59363824e-04, 3.15251318e-03, 9.95888174e-01],\n",
       "       [4.17993546e-01, 5.78275859e-01, 3.73066426e-03],\n",
       "       [3.69764209e-01, 6.25889778e-01, 4.34598001e-03],\n",
       "       [9.81459737e-01, 1.26864752e-02, 5.85382711e-03],\n",
       "       [9.99431312e-01, 1.12103829e-04, 4.56554932e-04],\n",
       "       [4.56932094e-06, 8.05278076e-04, 9.99190152e-01],\n",
       "       [4.68933322e-05, 1.72741100e-01, 8.27211976e-01],\n",
       "       [2.86119618e-03, 9.97119665e-01, 1.91088511e-05],\n",
       "       [1.14767561e-02, 9.69015528e-03, 9.78833020e-01],\n",
       "       [2.29075395e-05, 4.67350446e-02, 9.53242004e-01],\n",
       "       [2.22781897e-02, 2.89848085e-05, 9.77692783e-01],\n",
       "       [3.52982461e-04, 9.99369085e-01, 2.77990883e-04],\n",
       "       [8.92736733e-01, 8.63577276e-02, 2.09055413e-02],\n",
       "       [7.68544327e-04, 9.63012695e-01, 3.62187214e-02],\n",
       "       [1.03103876e-01, 3.63108456e-01, 5.33787668e-01],\n",
       "       [3.52374688e-02, 8.50284636e-01, 1.14477947e-01],\n",
       "       [2.47699127e-01, 1.34713486e-01, 6.17587388e-01],\n",
       "       [9.02891338e-01, 8.74264762e-02, 9.68226884e-03],\n",
       "       [7.79379189e-01, 2.17702001e-01, 2.91882921e-03],\n",
       "       [9.56928968e-01, 4.19642255e-02, 1.10677001e-03],\n",
       "       [6.80788001e-03, 9.92525578e-01, 6.66440639e-04],\n",
       "       [7.52850762e-03, 2.84151942e-01, 7.08319545e-01],\n",
       "       [7.97601941e-04, 9.96156275e-01, 3.04610631e-03],\n",
       "       [1.24254420e-01, 8.25291932e-01, 5.04536517e-02],\n",
       "       [4.03273942e-10, 1.52166152e-07, 9.99999881e-01],\n",
       "       [9.73243296e-01, 1.09959710e-02, 1.57607161e-02],\n",
       "       [6.93486810e-01, 3.04539084e-01, 1.97404274e-03],\n",
       "       [9.30377841e-02, 3.47319812e-01, 5.59642375e-01],\n",
       "       [1.10647511e-02, 9.85554457e-01, 3.38081387e-03],\n",
       "       [9.22045112e-03, 6.67979538e-01, 3.22800040e-01],\n",
       "       [1.12605192e-01, 4.60830256e-02, 8.41311812e-01],\n",
       "       [1.54548502e-02, 7.71165546e-03, 9.76833463e-01],\n",
       "       [3.33846137e-02, 9.66542065e-01, 7.32376138e-05],\n",
       "       [5.22542105e-05, 2.36406061e-03, 9.97583628e-01],\n",
       "       [2.33897977e-02, 9.75620687e-01, 9.89524880e-04],\n",
       "       [1.22754173e-02, 9.87715900e-01, 8.68192001e-06],\n",
       "       [7.82830417e-02, 2.04311311e-03, 9.19673800e-01],\n",
       "       [9.84778511e-04, 9.60763335e-01, 3.82519923e-02],\n",
       "       [2.28155841e-06, 7.00306055e-06, 9.99990702e-01],\n",
       "       [4.26625076e-04, 9.94955838e-01, 4.61758487e-03],\n",
       "       [9.98799920e-01, 1.09844678e-03, 1.01546459e-04],\n",
       "       [7.92333867e-06, 9.99979973e-01, 1.21354478e-05],\n",
       "       [2.48272717e-01, 6.41088545e-01, 1.10638775e-01],\n",
       "       [3.95691814e-03, 8.06428015e-01, 1.89615041e-01],\n",
       "       [1.10975243e-01, 7.06286252e-01, 1.82738483e-01],\n",
       "       [4.74827975e-04, 9.99503613e-01, 2.16166663e-05],\n",
       "       [9.96878278e-03, 1.21372612e-02, 9.77893949e-01],\n",
       "       [6.06433451e-01, 3.93498302e-01, 6.82151367e-05],\n",
       "       [5.25228534e-05, 9.94686127e-01, 5.26137697e-03],\n",
       "       [5.36853404e-05, 6.41964434e-04, 9.99304295e-01],\n",
       "       [7.32026165e-05, 9.99780118e-01, 1.46682156e-04],\n",
       "       [3.05048615e-01, 3.09062153e-01, 3.85889232e-01],\n",
       "       [1.81001611e-02, 6.99523449e-01, 2.82376379e-01],\n",
       "       [3.17505896e-01, 1.24131637e-02, 6.70080960e-01],\n",
       "       [4.78094335e-05, 1.28229713e-05, 9.99939322e-01],\n",
       "       [9.97028649e-01, 2.46158656e-04, 2.72522657e-03],\n",
       "       [5.33393845e-02, 8.98984790e-01, 4.76757549e-02],\n",
       "       [7.60221724e-07, 3.25619653e-02, 9.67437267e-01],\n",
       "       [6.63050532e-01, 3.26784968e-01, 1.01644862e-02],\n",
       "       [2.55212963e-01, 7.44689703e-01, 9.72875132e-05],\n",
       "       [4.96539940e-07, 6.80984755e-04, 9.99318480e-01],\n",
       "       [9.74512100e-01, 1.93801534e-04, 2.52941437e-02],\n",
       "       [8.96471925e-03, 7.89294899e-01, 2.01740339e-01],\n",
       "       [9.31855175e-05, 8.25940166e-03, 9.91647363e-01],\n",
       "       [8.08499346e-04, 1.42686993e-01, 8.56504560e-01],\n",
       "       [3.61048384e-04, 2.13282323e-03, 9.97506201e-01],\n",
       "       [1.16727993e-01, 1.20974600e-01, 7.62297392e-01],\n",
       "       [8.53337049e-01, 1.44755572e-01, 1.90731802e-03],\n",
       "       [9.53848734e-02, 9.02190626e-01, 2.42447248e-03],\n",
       "       [7.41149051e-07, 1.89832687e-01, 8.10166538e-01],\n",
       "       [6.03194523e-04, 9.76681471e-01, 2.27152221e-02],\n",
       "       [3.03762201e-02, 9.67462838e-01, 2.16100411e-03],\n",
       "       [4.29287031e-02, 1.10962436e-01, 8.46108794e-01],\n",
       "       [7.81408697e-02, 9.21519935e-01, 3.39207269e-04],\n",
       "       [9.26770866e-01, 7.24265054e-02, 8.02597264e-04],\n",
       "       [1.75309979e-05, 5.33524784e-04, 9.99448955e-01],\n",
       "       [4.77179390e-04, 9.82436538e-01, 1.70863811e-02],\n",
       "       [2.48354152e-02, 8.30299675e-01, 1.44864887e-01],\n",
       "       [3.79077694e-03, 9.95680213e-01, 5.29000186e-04],\n",
       "       [4.41719312e-05, 9.99497533e-01, 4.58258990e-04],\n",
       "       [2.72867620e-01, 3.90054792e-01, 3.37077618e-01],\n",
       "       [9.96285235e-04, 3.19406055e-02, 9.67063069e-01],\n",
       "       [9.03048273e-03, 9.65632319e-01, 2.53372807e-02],\n",
       "       [3.86888027e-01, 5.74962556e-01, 3.81493866e-02],\n",
       "       [7.59252757e-02, 9.21914279e-01, 2.16045417e-03],\n",
       "       [1.42313540e-03, 7.83212900e-01, 2.15363875e-01],\n",
       "       [9.98040617e-01, 1.54725474e-03, 4.12142486e-04],\n",
       "       [1.27649241e-06, 2.32756183e-01, 7.67242551e-01],\n",
       "       [9.18831408e-01, 4.20992859e-02, 3.90692651e-02],\n",
       "       [2.01233361e-05, 9.87190723e-01, 1.27891488e-02],\n",
       "       [1.05346016e-08, 6.19317812e-04, 9.99380589e-01]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
